{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1dd948652ed642939e4d0d2fb8561119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6559ace3f3694ec29dd13e55b6e374e7",
              "IPY_MODEL_0339373b835e44d9b6b0379bf8aad72b",
              "IPY_MODEL_6e06bc9d1c3f4d75aedc7467638555f1",
              "IPY_MODEL_5f25be5615de47669db84b8acdfca310",
              "IPY_MODEL_2b683052534a4e988c38a62885740ee8"
            ],
            "layout": "IPY_MODEL_66050e7fbb5042959b77e838d81dd4a8"
          }
        },
        "6559ace3f3694ec29dd13e55b6e374e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cf7bdf38fff471c92a579e0a8d8a63b",
            "placeholder": "​",
            "style": "IPY_MODEL_367f58263c394b74b45ded5f7e8892a5",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "0339373b835e44d9b6b0379bf8aad72b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_3df24a730d884819b0961fce2807e22a",
            "placeholder": "​",
            "style": "IPY_MODEL_cad89697eb4e463fa46a5aeab86f6bfd",
            "value": ""
          }
        },
        "6e06bc9d1c3f4d75aedc7467638555f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_6cd89011688a404da907d02b9c25f1cc",
            "style": "IPY_MODEL_dc7fb45b510544bfb5332a560cd51889",
            "value": true
          }
        },
        "5f25be5615de47669db84b8acdfca310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_412d4fb63fcc4aa5ab4507056f273f3d",
            "style": "IPY_MODEL_c203b00004c54c31aca1415a54e07eed",
            "tooltip": ""
          }
        },
        "2b683052534a4e988c38a62885740ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84da1c2f480b4a99b63b0d2947bcdd64",
            "placeholder": "​",
            "style": "IPY_MODEL_8a2a6f69a4e844368e8f99e399975e3e",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "66050e7fbb5042959b77e838d81dd4a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "7cf7bdf38fff471c92a579e0a8d8a63b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "367f58263c394b74b45ded5f7e8892a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3df24a730d884819b0961fce2807e22a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cad89697eb4e463fa46a5aeab86f6bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cd89011688a404da907d02b9c25f1cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc7fb45b510544bfb5332a560cd51889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "412d4fb63fcc4aa5ab4507056f273f3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c203b00004c54c31aca1415a54e07eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "84da1c2f480b4a99b63b0d2947bcdd64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a2a6f69a4e844368e8f99e399975e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43c5409081834f3890155bdad8dff429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd33092f9f134383bdc127fc315c1783",
              "IPY_MODEL_0510c7c3b4e84bd5a3d248a4824e1108",
              "IPY_MODEL_aafee70587e341248cfaedb1665d9ad1"
            ],
            "layout": "IPY_MODEL_9b99ddb3f94b44fdb735fc8539328159"
          }
        },
        "cd33092f9f134383bdc127fc315c1783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69506c2d2ad6413fa7f3e6cf2df3ed0b",
            "placeholder": "​",
            "style": "IPY_MODEL_76451a23b32b489bb2efcf3a84ab5539",
            "value": "Generating train split: "
          }
        },
        "0510c7c3b4e84bd5a3d248a4824e1108": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca63e2715ed34934938822b1ff9d82b0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1afbdacea9634dd2b2ae78ee4dd3a088",
            "value": 1
          }
        },
        "aafee70587e341248cfaedb1665d9ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a040bb8ff03640bbab3f266780dcc601",
            "placeholder": "​",
            "style": "IPY_MODEL_ed97d74ebfba49688acf75d20925dbe7",
            "value": " 12460/0 [00:00&lt;00:00, 82201.93 examples/s]"
          }
        },
        "9b99ddb3f94b44fdb735fc8539328159": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69506c2d2ad6413fa7f3e6cf2df3ed0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76451a23b32b489bb2efcf3a84ab5539": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca63e2715ed34934938822b1ff9d82b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1afbdacea9634dd2b2ae78ee4dd3a088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a040bb8ff03640bbab3f266780dcc601": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed97d74ebfba49688acf75d20925dbe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "532afd2829254741a5a823cd895bac15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79fd0cb188434db5946f233522e70aaf",
              "IPY_MODEL_62b929b4c68747dc94d128d1908f9fb3",
              "IPY_MODEL_c2a1e1081e5048f88d1e5a83e827f9c9",
              "IPY_MODEL_ab584ddcd4ce44e88b5e555e4f25deec",
              "IPY_MODEL_ad487ac01ab64ed394fa1e61deb5b8d6"
            ],
            "layout": "IPY_MODEL_9cd758abeac744b494afa433c5a3f0cf"
          }
        },
        "79fd0cb188434db5946f233522e70aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_501267e9189a4a68906b61191d1e098d",
            "placeholder": "​",
            "style": "IPY_MODEL_0e8d7585e2904747b7f3a491f5ca208c",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "62b929b4c68747dc94d128d1908f9fb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_d4a26c459a6a475884c3e4498ba2bc05",
            "placeholder": "​",
            "style": "IPY_MODEL_57499867136b4d4db6f235fddd46f229",
            "value": ""
          }
        },
        "c2a1e1081e5048f88d1e5a83e827f9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_b4fce9f5259b4b0c89a712f726b33b34",
            "style": "IPY_MODEL_fc640380c7c6473fb6efbc2a536ad0fa",
            "value": true
          }
        },
        "ab584ddcd4ce44e88b5e555e4f25deec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_9d42a7b0fcf4463495d0ac42906b534b",
            "style": "IPY_MODEL_b0e4025bff624e9fba7377fb8b277b9f",
            "tooltip": ""
          }
        },
        "ad487ac01ab64ed394fa1e61deb5b8d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f1095c3baf94aa8bb9f33a63a2a958f",
            "placeholder": "​",
            "style": "IPY_MODEL_0da89c8acb774605aef2938b12ee26a5",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "9cd758abeac744b494afa433c5a3f0cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "501267e9189a4a68906b61191d1e098d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e8d7585e2904747b7f3a491f5ca208c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4a26c459a6a475884c3e4498ba2bc05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57499867136b4d4db6f235fddd46f229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4fce9f5259b4b0c89a712f726b33b34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc640380c7c6473fb6efbc2a536ad0fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d42a7b0fcf4463495d0ac42906b534b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0e4025bff624e9fba7377fb8b277b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "9f1095c3baf94aa8bb9f33a63a2a958f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0da89c8acb774605aef2938b12ee26a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4f012e587c74f6e8c2a2a165ef5b87c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_abfa3a531c8648c99273da2fd32daff2",
              "IPY_MODEL_5844c468043648a0b50a4c17908062bf",
              "IPY_MODEL_b848b8367ef04c56b7794a420b4a4a25",
              "IPY_MODEL_9e9f28f32d2745a19ee7eb156c2af38f",
              "IPY_MODEL_621af5f496bb4433b27f83d68165c77d"
            ],
            "layout": "IPY_MODEL_d8570bf090624e8db7ca2c7270e4b406"
          }
        },
        "abfa3a531c8648c99273da2fd32daff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bab279c72b8c4174ba381165e4ed9f5c",
            "placeholder": "​",
            "style": "IPY_MODEL_01d9beacf6a94ef1aecc840c235f250f",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "5844c468043648a0b50a4c17908062bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_6ce1dd3fd9eb4e26898b0b0cde989e2b",
            "placeholder": "​",
            "style": "IPY_MODEL_a3985fc366444a4f82082b2f8ca1128d",
            "value": ""
          }
        },
        "b848b8367ef04c56b7794a420b4a4a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_0752895c73264f9aaa8303ccab0a7020",
            "style": "IPY_MODEL_7ce9eb93981449549a2b92adbba6487f",
            "value": true
          }
        },
        "9e9f28f32d2745a19ee7eb156c2af38f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_15c8fca089c04af5abae8856f0d5181b",
            "style": "IPY_MODEL_1ce374a71014461e8033ef76c44f379a",
            "tooltip": ""
          }
        },
        "621af5f496bb4433b27f83d68165c77d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe8f623a587148beaba856fd360d91e1",
            "placeholder": "​",
            "style": "IPY_MODEL_b890b397cec64bb29da45d1a4cb22a83",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "d8570bf090624e8db7ca2c7270e4b406": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "bab279c72b8c4174ba381165e4ed9f5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01d9beacf6a94ef1aecc840c235f250f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ce1dd3fd9eb4e26898b0b0cde989e2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3985fc366444a4f82082b2f8ca1128d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0752895c73264f9aaa8303ccab0a7020": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ce9eb93981449549a2b92adbba6487f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15c8fca089c04af5abae8856f0d5181b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ce374a71014461e8033ef76c44f379a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "fe8f623a587148beaba856fd360d91e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b890b397cec64bb29da45d1a4cb22a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02f28fc999ec4078b7dfabde2e356d75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f6146cc0d94428cad75e4be67b65601",
              "IPY_MODEL_2ada8395d184459da5894485cdba0655",
              "IPY_MODEL_e49d89d8e2b04b2087646c36f8d87072"
            ],
            "layout": "IPY_MODEL_c157711df0374052bce756b1f5ca324c"
          }
        },
        "5f6146cc0d94428cad75e4be67b65601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_483e96021efb48c781db27a2f0fc98de",
            "placeholder": "​",
            "style": "IPY_MODEL_59d80396ea8044d5a05c3061a42a3929",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "2ada8395d184459da5894485cdba0655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fc92232ea094e9f821740bc91feced5",
            "max": 3071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c696a6b83424785bd66bf8b6edee294",
            "value": 3071
          }
        },
        "e49d89d8e2b04b2087646c36f8d87072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b7920bca00d4a88858bf52a0dd478bc",
            "placeholder": "​",
            "style": "IPY_MODEL_325d335636bb444b8985af2b3c086be7",
            "value": " 3.07k/3.07k [00:00&lt;00:00, 345kB/s]"
          }
        },
        "c157711df0374052bce756b1f5ca324c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "483e96021efb48c781db27a2f0fc98de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59d80396ea8044d5a05c3061a42a3929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fc92232ea094e9f821740bc91feced5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c696a6b83424785bd66bf8b6edee294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b7920bca00d4a88858bf52a0dd478bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "325d335636bb444b8985af2b3c086be7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e02a949c0ea84ae4a181b2f6b12c4fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48226eda7ce04d1cb71c1fc145bf52be",
              "IPY_MODEL_cf5ed70d1d6f40b9bee28f4926d77beb",
              "IPY_MODEL_b340af91fe9d4d318feb6b133dc621e0"
            ],
            "layout": "IPY_MODEL_6685dfb871634f3e9067904cf6d7b2d1"
          }
        },
        "48226eda7ce04d1cb71c1fc145bf52be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e92857ba69a14701bebe94c68e659f0f",
            "placeholder": "​",
            "style": "IPY_MODEL_64c48dc106fa4d0090bd919e2eb917df",
            "value": "tokenizer.json: 100%"
          }
        },
        "cf5ed70d1d6f40b9bee28f4926d77beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3271af65181d4d318ab062e438c20c3d",
            "max": 7031660,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_751d4c06aa4c42da87fa2eef13f39f36",
            "value": 7031660
          }
        },
        "b340af91fe9d4d318feb6b133dc621e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4509ae513b9413988231ab44fce81c9",
            "placeholder": "​",
            "style": "IPY_MODEL_313346b73e744d609d6445c09c114716",
            "value": " 7.03M/7.03M [00:00&lt;00:00, 8.27MB/s]"
          }
        },
        "6685dfb871634f3e9067904cf6d7b2d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e92857ba69a14701bebe94c68e659f0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64c48dc106fa4d0090bd919e2eb917df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3271af65181d4d318ab062e438c20c3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "751d4c06aa4c42da87fa2eef13f39f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4509ae513b9413988231ab44fce81c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "313346b73e744d609d6445c09c114716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c007d150634c46ce954239eb24a9c19d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48c4754bd58146398e4b4baf25a38ef6",
              "IPY_MODEL_1a98a601a82c4666a1cb999e0e7cf0ff",
              "IPY_MODEL_47d9ce657e2a45799956e0d311548d12"
            ],
            "layout": "IPY_MODEL_9d1e49767da54052a41daa5a97637534"
          }
        },
        "48c4754bd58146398e4b4baf25a38ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bb795f624ea4a1188fa35043dc4bf58",
            "placeholder": "​",
            "style": "IPY_MODEL_d2f8336310e842a6aff8cea961ed9436",
            "value": "Map: 100%"
          }
        },
        "1a98a601a82c4666a1cb999e0e7cf0ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4125e2afa86b430e8b81d67bfb6dd5c0",
            "max": 12460,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_500b0b229f8f4d60b342bcd51c1a71c4",
            "value": 12460
          }
        },
        "47d9ce657e2a45799956e0d311548d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4d9b85fed8f4d399da9358810399468",
            "placeholder": "​",
            "style": "IPY_MODEL_303c63fedcb94f06af29c03badf7e982",
            "value": " 12460/12460 [00:12&lt;00:00, 1022.23 examples/s]"
          }
        },
        "9d1e49767da54052a41daa5a97637534": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bb795f624ea4a1188fa35043dc4bf58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2f8336310e842a6aff8cea961ed9436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4125e2afa86b430e8b81d67bfb6dd5c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "500b0b229f8f4d60b342bcd51c1a71c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4d9b85fed8f4d399da9358810399468": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "303c63fedcb94f06af29c03badf7e982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec68c59b7ec4453eaef7a938a082a7ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fbf3d8a0dfa44489854b6ad5aa5519b2",
              "IPY_MODEL_645d1936582a4c8d867709567b9bbbfa",
              "IPY_MODEL_b91eccc4fa3c49b89cbd107f2951d275"
            ],
            "layout": "IPY_MODEL_a2c39ea3584b4ab396f4e6251797160c"
          }
        },
        "fbf3d8a0dfa44489854b6ad5aa5519b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f94e7f8deab64ce8a12ff00b20b8edcf",
            "placeholder": "​",
            "style": "IPY_MODEL_e70acc135e064204bf13bd04b89508e9",
            "value": "Map: 100%"
          }
        },
        "645d1936582a4c8d867709567b9bbbfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40515c0214114c2684e5d0aac97095b7",
            "max": 12460,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7c4f167e4104f7faae4038bfe2fc8fa",
            "value": 12460
          }
        },
        "b91eccc4fa3c49b89cbd107f2951d275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d89e86e95c14ca18bd37096f7d6050d",
            "placeholder": "​",
            "style": "IPY_MODEL_06aaca97ce79453a9ec01a13c03419bf",
            "value": " 12460/12460 [00:05&lt;00:00, 2309.02 examples/s]"
          }
        },
        "a2c39ea3584b4ab396f4e6251797160c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f94e7f8deab64ce8a12ff00b20b8edcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e70acc135e064204bf13bd04b89508e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40515c0214114c2684e5d0aac97095b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7c4f167e4104f7faae4038bfe2fc8fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d89e86e95c14ca18bd37096f7d6050d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06aaca97ce79453a9ec01a13c03419bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n7jstpBlnQ_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMs9uNDMHL6R"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,TrainingArguments\n",
        "from peft import LoraConfig\n",
        "import torch\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpfOyXaynYwg",
        "outputId": "9286564f-3b16-4817-f798-75d7331cc0cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.3.0)\n",
            "Requirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.48.3)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.28.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.21.0->trl) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (3.11.13)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (0.21.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWBFYqn-WQ8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb957f27-97f0-457d-eae7-04a21de5e32b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "file_path = '/content/drive/My Drive/dataset_finetuning/train.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dvLj8sDgTiQk",
        "outputId": "5e80a1ee-37a1-4d04-f5a1-29f219b5c5fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                           dialogue  \\\n",
              "0  train_0  #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...   \n",
              "1  train_1  #Person1#: Hello Mrs. Parker, how have you bee...   \n",
              "2  train_2  #Person1#: Excuse me, did you see a set of key...   \n",
              "3  train_3  #Person1#: Why didn't you tell me you had a gi...   \n",
              "4  train_4  #Person1#: Watsup, ladies! Y'll looking'fine t...   \n",
              "\n",
              "                                             summary              topic  \n",
              "0  Mr. Smith's getting a check-up, and Doctor Haw...     get a check-up  \n",
              "1  Mrs Parker takes Ricky for his vaccines. Dr. P...           vaccines  \n",
              "2  #Person1#'s looking for a set of keys and asks...          find keys  \n",
              "3  #Person1#'s angry because #Person2# didn't tel...  have a girlfriend  \n",
              "4  Malik invites Nikki to dance. Nikki agrees if ...              dance  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-071d1ebd-22f4-4f48-a0a0-c0ce4ebe6d9c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>dialogue</th>\n",
              "      <th>summary</th>\n",
              "      <th>topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train_0</td>\n",
              "      <td>#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...</td>\n",
              "      <td>Mr. Smith's getting a check-up, and Doctor Haw...</td>\n",
              "      <td>get a check-up</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train_1</td>\n",
              "      <td>#Person1#: Hello Mrs. Parker, how have you bee...</td>\n",
              "      <td>Mrs Parker takes Ricky for his vaccines. Dr. P...</td>\n",
              "      <td>vaccines</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train_2</td>\n",
              "      <td>#Person1#: Excuse me, did you see a set of key...</td>\n",
              "      <td>#Person1#'s looking for a set of keys and asks...</td>\n",
              "      <td>find keys</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train_3</td>\n",
              "      <td>#Person1#: Why didn't you tell me you had a gi...</td>\n",
              "      <td>#Person1#'s angry because #Person2# didn't tel...</td>\n",
              "      <td>have a girlfriend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train_4</td>\n",
              "      <td>#Person1#: Watsup, ladies! Y'll looking'fine t...</td>\n",
              "      <td>Malik invites Nikki to dance. Nikki agrees if ...</td>\n",
              "      <td>dance</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-071d1ebd-22f4-4f48-a0a0-c0ce4ebe6d9c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-071d1ebd-22f4-4f48-a0a0-c0ce4ebe6d9c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-071d1ebd-22f4-4f48-a0a0-c0ce4ebe6d9c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8f806444-be55-4bbc-9a40-f56d182b5bce\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8f806444-be55-4bbc-9a40-f56d182b5bce')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8f806444-be55-4bbc-9a40-f56d182b5bce button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 12460,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12460,\n        \"samples\": [\n          \"train_10562\",\n          \"train_9191\",\n          \"train_5556\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dialogue\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12458,\n        \"samples\": [\n          \"#Person1#: Hello, is Jack there?\\n#Person2#: Speaking.\\n#Person1#: Jack! It's Rose here.\\n#Person2#: Hi, Rose. How's everything?\\n#Person1#: Fine, thanks. I'm having several friends over for dinner this Saturday. And I was wondering if you have the time to join us.\\n#Person2#: Sounds good. What time do you want me to come?\\n#Person1#: Is six o'clock okay?\",\n          \"#Person1#: Excuse me. What should I wear if I want to go to an interview?\\n#Person2#: You should wear a tie to go with your suit.\\n#Person1#: I am afraid I would tense up during the interview.\\n#Person2#: It doesn't matter. Just do your best to sell yourself.\",\n          \"#Person1#: Now, I'm going to start off by asking you a difficult question. Why would you like to get this post?\\n#Person2#: Well, first of all I know that your firm has a very good reputation. Then I've heard you offer good opportunities for promotion for the right person.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12436,\n        \"samples\": [\n          \"#Person1# planned to read some articles that are on reserve in the library but all copies have been checked out. #Person2# is assisting #Person1# with this and advises him to wait earlier the next morning.\",\n          \"#Person2# wants to buy some Chinese-style cakes. #Person1# recommends moon cakes and tells #Person2# the differences between Cantonese style and Suzhou style. #Person2# buys some of each.\",\n          \"Emily tells #Person1# she had a horrible day. Her manager made some mistakes and blamed them on her. Emily has no idea why her manager treats her worse than others but she can do nothing until she gets a promotion. #Person1# thinks that's sensible.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"topic\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7434,\n        \"samples\": [\n          \"blackout\",\n          \"money and health\",\n          \"coffee\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['id','topic'])\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "u0jStBclUvUq",
        "outputId": "79d19d78-f306-4a7b-bfb0-f0367acf6987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            dialogue  \\\n",
              "0  #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...   \n",
              "1  #Person1#: Hello Mrs. Parker, how have you bee...   \n",
              "2  #Person1#: Excuse me, did you see a set of key...   \n",
              "3  #Person1#: Why didn't you tell me you had a gi...   \n",
              "4  #Person1#: Watsup, ladies! Y'll looking'fine t...   \n",
              "\n",
              "                                             summary  \n",
              "0  Mr. Smith's getting a check-up, and Doctor Haw...  \n",
              "1  Mrs Parker takes Ricky for his vaccines. Dr. P...  \n",
              "2  #Person1#'s looking for a set of keys and asks...  \n",
              "3  #Person1#'s angry because #Person2# didn't tel...  \n",
              "4  Malik invites Nikki to dance. Nikki agrees if ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-63d55235-589e-4884-b7a1-cbb912a86e8b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dialogue</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...</td>\n",
              "      <td>Mr. Smith's getting a check-up, and Doctor Haw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#Person1#: Hello Mrs. Parker, how have you bee...</td>\n",
              "      <td>Mrs Parker takes Ricky for his vaccines. Dr. P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#Person1#: Excuse me, did you see a set of key...</td>\n",
              "      <td>#Person1#'s looking for a set of keys and asks...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#Person1#: Why didn't you tell me you had a gi...</td>\n",
              "      <td>#Person1#'s angry because #Person2# didn't tel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>#Person1#: Watsup, ladies! Y'll looking'fine t...</td>\n",
              "      <td>Malik invites Nikki to dance. Nikki agrees if ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-63d55235-589e-4884-b7a1-cbb912a86e8b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-63d55235-589e-4884-b7a1-cbb912a86e8b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-63d55235-589e-4884-b7a1-cbb912a86e8b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c25b43c5-2809-4517-9436-917b00f2eec1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c25b43c5-2809-4517-9436-917b00f2eec1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c25b43c5-2809-4517-9436-917b00f2eec1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 12460,\n  \"fields\": [\n    {\n      \"column\": \"dialogue\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12458,\n        \"samples\": [\n          \"#Person1#: Hello, is Jack there?\\n#Person2#: Speaking.\\n#Person1#: Jack! It's Rose here.\\n#Person2#: Hi, Rose. How's everything?\\n#Person1#: Fine, thanks. I'm having several friends over for dinner this Saturday. And I was wondering if you have the time to join us.\\n#Person2#: Sounds good. What time do you want me to come?\\n#Person1#: Is six o'clock okay?\",\n          \"#Person1#: Excuse me. What should I wear if I want to go to an interview?\\n#Person2#: You should wear a tie to go with your suit.\\n#Person1#: I am afraid I would tense up during the interview.\\n#Person2#: It doesn't matter. Just do your best to sell yourself.\",\n          \"#Person1#: Now, I'm going to start off by asking you a difficult question. Why would you like to get this post?\\n#Person2#: Well, first of all I know that your firm has a very good reputation. Then I've heard you offer good opportunities for promotion for the right person.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12436,\n        \"samples\": [\n          \"#Person1# planned to read some articles that are on reserve in the library but all copies have been checked out. #Person2# is assisting #Person1# with this and advises him to wait earlier the next morning.\",\n          \"#Person2# wants to buy some Chinese-style cakes. #Person1# recommends moon cakes and tells #Person2# the differences between Cantonese style and Suzhou style. #Person2# buys some of each.\",\n          \"Emily tells #Person1# she had a horrible day. Her manager made some mistakes and blamed them on her. Emily has no idea why her manager treats her worse than others but she can do nothing until she gets a promotion. #Person1# thinks that's sensible.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns = {'dialogue':'input', 'summary':'output'}, inplace = True)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "xacfwuZHVTpn",
        "outputId": "b2b3dbc2-a55e-467b-cf04-26d3ddec1ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               input  \\\n",
              "0  #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...   \n",
              "1  #Person1#: Hello Mrs. Parker, how have you bee...   \n",
              "2  #Person1#: Excuse me, did you see a set of key...   \n",
              "3  #Person1#: Why didn't you tell me you had a gi...   \n",
              "4  #Person1#: Watsup, ladies! Y'll looking'fine t...   \n",
              "\n",
              "                                              output  \n",
              "0  Mr. Smith's getting a check-up, and Doctor Haw...  \n",
              "1  Mrs Parker takes Ricky for his vaccines. Dr. P...  \n",
              "2  #Person1#'s looking for a set of keys and asks...  \n",
              "3  #Person1#'s angry because #Person2# didn't tel...  \n",
              "4  Malik invites Nikki to dance. Nikki agrees if ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-765b8192-a841-4e59-a46b-1b6802cbc138\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...</td>\n",
              "      <td>Mr. Smith's getting a check-up, and Doctor Haw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#Person1#: Hello Mrs. Parker, how have you bee...</td>\n",
              "      <td>Mrs Parker takes Ricky for his vaccines. Dr. P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#Person1#: Excuse me, did you see a set of key...</td>\n",
              "      <td>#Person1#'s looking for a set of keys and asks...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#Person1#: Why didn't you tell me you had a gi...</td>\n",
              "      <td>#Person1#'s angry because #Person2# didn't tel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>#Person1#: Watsup, ladies! Y'll looking'fine t...</td>\n",
              "      <td>Malik invites Nikki to dance. Nikki agrees if ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-765b8192-a841-4e59-a46b-1b6802cbc138')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-765b8192-a841-4e59-a46b-1b6802cbc138 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-765b8192-a841-4e59-a46b-1b6802cbc138');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e988d3a6-73fa-4673-8eeb-f58b478cc975\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e988d3a6-73fa-4673-8eeb-f58b478cc975')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e988d3a6-73fa-4673-8eeb-f58b478cc975 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 12460,\n  \"fields\": [\n    {\n      \"column\": \"input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12458,\n        \"samples\": [\n          \"#Person1#: Hello, is Jack there?\\n#Person2#: Speaking.\\n#Person1#: Jack! It's Rose here.\\n#Person2#: Hi, Rose. How's everything?\\n#Person1#: Fine, thanks. I'm having several friends over for dinner this Saturday. And I was wondering if you have the time to join us.\\n#Person2#: Sounds good. What time do you want me to come?\\n#Person1#: Is six o'clock okay?\",\n          \"#Person1#: Excuse me. What should I wear if I want to go to an interview?\\n#Person2#: You should wear a tie to go with your suit.\\n#Person1#: I am afraid I would tense up during the interview.\\n#Person2#: It doesn't matter. Just do your best to sell yourself.\",\n          \"#Person1#: Now, I'm going to start off by asking you a difficult question. Why would you like to get this post?\\n#Person2#: Well, first of all I know that your firm has a very good reputation. Then I've heard you offer good opportunities for promotion for the right person.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12436,\n        \"samples\": [\n          \"#Person1# planned to read some articles that are on reserve in the library but all copies have been checked out. #Person2# is assisting #Person1# with this and advises him to wait earlier the next morning.\",\n          \"#Person2# wants to buy some Chinese-style cakes. #Person1# recommends moon cakes and tells #Person2# the differences between Cantonese style and Suzhou style. #Person2# buys some of each.\",\n          \"Emily tells #Person1# she had a horrible day. Her manager made some mistakes and blamed them on her. Emily has no idea why her manager treats her worse than others but she can do nothing until she gets a promotion. #Person1# thinks that's sensible.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358,
          "referenced_widgets": [
            "1dd948652ed642939e4d0d2fb8561119",
            "6559ace3f3694ec29dd13e55b6e374e7",
            "0339373b835e44d9b6b0379bf8aad72b",
            "6e06bc9d1c3f4d75aedc7467638555f1",
            "5f25be5615de47669db84b8acdfca310",
            "2b683052534a4e988c38a62885740ee8",
            "66050e7fbb5042959b77e838d81dd4a8",
            "7cf7bdf38fff471c92a579e0a8d8a63b",
            "367f58263c394b74b45ded5f7e8892a5",
            "3df24a730d884819b0961fce2807e22a",
            "cad89697eb4e463fa46a5aeab86f6bfd",
            "6cd89011688a404da907d02b9c25f1cc",
            "dc7fb45b510544bfb5332a560cd51889",
            "412d4fb63fcc4aa5ab4507056f273f3d",
            "c203b00004c54c31aca1415a54e07eed",
            "84da1c2f480b4a99b63b0d2947bcdd64",
            "8a2a6f69a4e844368e8f99e399975e3e"
          ]
        },
        "id": "1RWybR7UeF-2",
        "outputId": "05e86aeb-cb00-447f-e0a4-a6b6ae41b3dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1dd948652ed642939e4d0d2fb8561119"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "data = [\n",
        "    {\"input_text\": row[\"input\"], \"target_text\": row[\"output\"]}\n",
        "    for _, row in df.iterrows()\n",
        "]\n",
        "json_file_path = \"/content/drive/My Drive/dataset_finetuning/train.json\"\n",
        "with open(json_file_path, \"w\",encoding= \"utf-8\") as json_file:\n",
        "    json.dump(data, json_file, indent=4)\n",
        "\n",
        "print(f\"JSON file saved to {json_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk0sVOH4XHvV",
        "outputId": "b967c844-103d-41d1-dc0a-6b3b431de1c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON file saved to /content/drive/My Drive/dataset_finetuning/train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists(json_file_path):\n",
        "    print(\"✅ JSON file saved\")\n",
        "else:\n",
        "    print(\"❌ JSON file not found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tltzw2jajJF6",
        "outputId": "d8351231-a090-4c80-f98e-e21eab083d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ JSON file saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6huhEsKXjTuo",
        "outputId": "ea34390b-09d8-4fd5-90ce-482848985439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "json_file_path = \"/content/drive/My Drive/dataset_finetuning/train.json\"\n",
        "dataset = load_dataset(\"json\", data_files=json_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "43c5409081834f3890155bdad8dff429",
            "cd33092f9f134383bdc127fc315c1783",
            "0510c7c3b4e84bd5a3d248a4824e1108",
            "aafee70587e341248cfaedb1665d9ad1",
            "9b99ddb3f94b44fdb735fc8539328159",
            "69506c2d2ad6413fa7f3e6cf2df3ed0b",
            "76451a23b32b489bb2efcf3a84ab5539",
            "ca63e2715ed34934938822b1ff9d82b0",
            "1afbdacea9634dd2b2ae78ee4dd3a088",
            "a040bb8ff03640bbab3f266780dcc601",
            "ed97d74ebfba49688acf75d20925dbe7"
          ]
        },
        "id": "bxo2tmnqjccq",
        "outputId": "779206d6-c42b-442d-d0de-26d05deaadaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43c5409081834f3890155bdad8dff429"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)\n",
        "print(dataset['train'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s675lF9kpa8",
        "outputId": "a1003716-dd2a-410c-bbd6-5379b73dc09d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_text', 'target_text'],\n",
            "        num_rows: 12460\n",
            "    })\n",
            "})\n",
            "{'input_text': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\", 'target_text': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358,
          "referenced_widgets": [
            "532afd2829254741a5a823cd895bac15",
            "79fd0cb188434db5946f233522e70aaf",
            "62b929b4c68747dc94d128d1908f9fb3",
            "c2a1e1081e5048f88d1e5a83e827f9c9",
            "ab584ddcd4ce44e88b5e555e4f25deec",
            "ad487ac01ab64ed394fa1e61deb5b8d6",
            "9cd758abeac744b494afa433c5a3f0cf",
            "501267e9189a4a68906b61191d1e098d",
            "0e8d7585e2904747b7f3a491f5ca208c",
            "d4a26c459a6a475884c3e4498ba2bc05",
            "57499867136b4d4db6f235fddd46f229",
            "b4fce9f5259b4b0c89a712f726b33b34",
            "fc640380c7c6473fb6efbc2a536ad0fa",
            "9d42a7b0fcf4463495d0ac42906b534b",
            "b0e4025bff624e9fba7377fb8b277b9f",
            "9f1095c3baf94aa8bb9f33a63a2a958f",
            "0da89c8acb774605aef2938b12ee26a5"
          ]
        },
        "id": "jmxCcmYPp6OI",
        "outputId": "a075bf80-7d37-4403-f4e6-3fe92b7fbb09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "532afd2829254741a5a823cd895bac15"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Hugging Face'e giriş yap\n",
        "notebook_login()\n",
        "\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)  # Güvenli giriş\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Chat modeline uygun hale getirme\n",
        "    messages = [\n",
        "        [{\"role\": \"user\", \"content\": input_text}, {\"role\": \"assistant\", \"content\": target_text}]\n",
        "        for input_text, target_text in zip(examples[\"input_text\"], examples[\"target_text\"])\n",
        "    ]\n",
        "\n",
        "    # Chat template uygula ve tokenize et\n",
        "    input_texts = [\n",
        "        tokenizer.apply_chat_template(m, tokenize=True, add_generation_prompt=True) for m in messages\n",
        "    ]\n",
        "\n",
        "    # Model için labels tokenizasyonu\n",
        "    labels = tokenizer(examples[\"target_text\"], max_length=256, truncation=True)\n",
        "\n",
        "    # Eğer tokenizer'ın dönüş değeri tensor ise, listeye çevir\n",
        "    if isinstance(input_texts[0], list):\n",
        "        input_ids = input_texts\n",
        "    else:\n",
        "        input_ids = [t.tolist() if hasattr(t, \"tolist\") else list(t) for t in input_texts]\n",
        "\n",
        "    return {\"input_ids\": input_ids, \"labels\": labels[\"input_ids\"]}\n",
        "\n",
        "# Dataset'i batch halinde tokenize et\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454,
          "referenced_widgets": [
            "c4f012e587c74f6e8c2a2a165ef5b87c",
            "abfa3a531c8648c99273da2fd32daff2",
            "5844c468043648a0b50a4c17908062bf",
            "b848b8367ef04c56b7794a420b4a4a25",
            "9e9f28f32d2745a19ee7eb156c2af38f",
            "621af5f496bb4433b27f83d68165c77d",
            "d8570bf090624e8db7ca2c7270e4b406",
            "bab279c72b8c4174ba381165e4ed9f5c",
            "01d9beacf6a94ef1aecc840c235f250f",
            "6ce1dd3fd9eb4e26898b0b0cde989e2b",
            "a3985fc366444a4f82082b2f8ca1128d",
            "0752895c73264f9aaa8303ccab0a7020",
            "7ce9eb93981449549a2b92adbba6487f",
            "15c8fca089c04af5abae8856f0d5181b",
            "1ce374a71014461e8033ef76c44f379a",
            "fe8f623a587148beaba856fd360d91e1",
            "b890b397cec64bb29da45d1a4cb22a83",
            "02f28fc999ec4078b7dfabde2e356d75",
            "5f6146cc0d94428cad75e4be67b65601",
            "2ada8395d184459da5894485cdba0655",
            "e49d89d8e2b04b2087646c36f8d87072",
            "c157711df0374052bce756b1f5ca324c",
            "483e96021efb48c781db27a2f0fc98de",
            "59d80396ea8044d5a05c3061a42a3929",
            "7fc92232ea094e9f821740bc91feced5",
            "4c696a6b83424785bd66bf8b6edee294",
            "0b7920bca00d4a88858bf52a0dd478bc",
            "325d335636bb444b8985af2b3c086be7",
            "e02a949c0ea84ae4a181b2f6b12c4fb6",
            "48226eda7ce04d1cb71c1fc145bf52be",
            "cf5ed70d1d6f40b9bee28f4926d77beb",
            "b340af91fe9d4d318feb6b133dc621e0",
            "6685dfb871634f3e9067904cf6d7b2d1",
            "e92857ba69a14701bebe94c68e659f0f",
            "64c48dc106fa4d0090bd919e2eb917df",
            "3271af65181d4d318ab062e438c20c3d",
            "751d4c06aa4c42da87fa2eef13f39f36",
            "f4509ae513b9413988231ab44fce81c9",
            "313346b73e744d609d6445c09c114716",
            "c007d150634c46ce954239eb24a9c19d",
            "48c4754bd58146398e4b4baf25a38ef6",
            "1a98a601a82c4666a1cb999e0e7cf0ff",
            "47d9ce657e2a45799956e0d311548d12",
            "9d1e49767da54052a41daa5a97637534",
            "3bb795f624ea4a1188fa35043dc4bf58",
            "d2f8336310e842a6aff8cea961ed9436",
            "4125e2afa86b430e8b81d67bfb6dd5c0",
            "500b0b229f8f4d60b342bcd51c1a71c4",
            "f4d9b85fed8f4d399da9358810399468",
            "303c63fedcb94f06af29c03badf7e982"
          ]
        },
        "id": "0s6OZkh2lFnh",
        "outputId": "353d0a9b-372b-4392-f15c-104441a7e2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4f012e587c74f6e8c2a2a165ef5b87c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02f28fc999ec4078b7dfabde2e356d75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e02a949c0ea84ae4a181b2f6b12c4fb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c007d150634c46ce954239eb24a9c19d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"train\"][0])"
      ],
      "metadata": {
        "id": "967ibOyZp10D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b04e23c-e295-467a-dbf0-b4cbd48053c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_text': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\", 'target_text': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(modesl_name, token=True)\n",
        "\n",
        "raw_input = tokenized_dataset[\"train\"][0][\"input_ids\"]\n",
        "input_text = tokenizer.decode(raw_input, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "formatted_input = f\"Can you summarize this conversation briefly?\\n\\n{input_text}\"\n",
        "\n",
        "\n",
        "input_ids = tokenizer(formatted_input, return_tensors=\"pt\").input_ids\n",
        "\n",
        "output_tokens = model.generate(input_ids, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
        "output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"🔹 **Eğitimsiz Modelin Özetlemesi:**\")\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dI3Sd2JydWW",
        "outputId": "2f738451-50bb-40d6-c64c-013c0e8f126a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Eğitimsiz Modelin Özetlemesi:**\n",
            "Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
            "#Person2#: I found it would be a good idea to get a check-up.\n",
            "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
            "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
            "#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n",
            "#Person2#: Ok.\n",
            "#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n",
            "#Person2#: Yes.\n",
            "#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n",
            "#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n",
            "#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n",
            "#Person2#: Ok, thanks doctor. I'm sorry I couldn't make it.\n",
            "#Person1#: No problem, I'm here all the time.\n",
            "#Person2#: Sorry, but I can't help you anymore.\n",
            "</think>\n",
            "\n",
            "**Summary:**\n",
            "\n",
            "Mr. Smith, a doctor, asks Dr. Hawkins about a check-up. Hawkins confirms he will visit soon, citing his health and a history of smoking. Dr. Hawkins discusses smoking's impact on lung cancer and heart disease, advising him to quit smoking. He apologizes for missing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3ZwMM6b0Ebs",
        "outputId": "fce01724-3243-4b15-bf1c-014613cd7f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
            "#Person2#: I found it would be a good idea to get a check-up.\n",
            "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
            "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
            "#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n",
            "#Person2#: Ok.\n",
            "#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n",
            "#Person2#: Yes.\n",
            "#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n",
            "#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n",
            "#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n",
            "#Person2#: Ok, thanks doctor.<｜Assistant｜>Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.<｜Assistant｜><think>\n",
            "Okay, so I'm trying to understand this conversation between Doctor Hawkins and Mr. Smith. I'm not a doctor or a student, but I want to make sense of what's happening here. Let me read through it again.\n",
            "\n",
            "So, Doctor Hawkins is here, and Mr. Smith is asking about a check-up. He says, \"Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\" That's straightforward. Then, he asks, \"I found it would be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers peft accelerate datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEW7BZ17098k",
        "outputId": "d903ac7e-50a0-4da6-8cf9-03a71be59c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./deepseek_finetuned\",  # Modelin kaydedileceği yer\n",
        "    per_device_train_batch_size=2,  # Her GPU'da kullanılacak batch boyutu\n",
        "    num_train_epochs=3,  # Kaç epoch eğiteceğiz?\n",
        "    save_steps=500,  # Kaç adımda bir modeli kaydedelim?\n",
        "    save_total_limit=2,  # Kaç tane checkpoint tutulsun?\n",
        "    logging_steps=50,  # Her 50 adımda bir log kaydı al\n",
        "    evaluation_strategy=\"epoch\",  # Her epoch sonunda değerlendirme yap\n",
        "    learning_rate=2e-4,  # Öğrenme oranı\n",
        "    weight_decay=0.01,  # Ağırlık bozulmasını engellemek için\n",
        "    bf16=True,  # GPU kullanıyorsan eğitimi hızlandırır\n",
        "    push_to_hub=False  # Eğer Hugging Face Hub'a kaydetmek istersen True yap\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBYulWdE1WJj",
        "outputId": "8f7a5b25-0750-4489-983d-9f8f684269ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Eğer Hugging Face'e dataset'i yüklediysen, buradan çağırabilirsin\n",
        "# dataset = load_dataset(\"bilge/aya-finetuning-dataset\")\n",
        "\n",
        "# Train/Test ayırımı yapalım (%80 train, %20 test)\n",
        "split_dataset = tokenized_dataset[\"train\"].train_test_split(test_size=0.2)\n",
        "\n",
        "# Ayırdığımız datasetleri görelim\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "test_dataset = split_dataset[\"test\"]\n",
        "\n",
        "print(f\"✅ Train Set Boyutu: {len(train_dataset)}\")\n",
        "print(f\"✅ Test Set Boyutu: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-YZd93H16_v",
        "outputId": "0a04a4e7-1299-4557-a6b1-bd54ef51762e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Train Set Boyutu: 9968\n",
            "✅ Test Set Boyutu: 2492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    # Prompt'u input_text'e ekleyelim\n",
        "    formatted_inputs = [f\"Can you summarize this conversation briefly?\\n\\n{text}\" for text in examples[\"input_text\"]]\n",
        "\n",
        "    # Chat formatına uygun hale getirme\n",
        "    messages = [\n",
        "        [{\"role\": \"user\", \"content\": input_text}, {\"role\": \"assistant\", \"content\": target_text}]\n",
        "        for input_text, target_text in zip(formatted_inputs, examples[\"target_text\"])\n",
        "    ]\n",
        "\n",
        "    # Chat template uygula ve tokenize et\n",
        "    input_texts = [tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True) for m in messages]\n",
        "\n",
        "    # Model için labels tokenizasyonu (LoRA sonrası bozulmaması için `return_tensors=\"pt\"` kaldırıldı)\n",
        "    inputs = tokenizer(\n",
        "        input_texts,\n",
        "        max_length=256,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    labels = tokenizer(\n",
        "        examples[\"target_text\"],\n",
        "        max_length=256,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": inputs[\"input_ids\"],\n",
        "        \"attention_mask\": inputs[\"attention_mask\"],\n",
        "        \"labels\": labels[\"input_ids\"]\n",
        "    }\n",
        "\n",
        "# Dataset'i tekrar tokenize edelim\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ec68c59b7ec4453eaef7a938a082a7ce",
            "fbf3d8a0dfa44489854b6ad5aa5519b2",
            "645d1936582a4c8d867709567b9bbbfa",
            "b91eccc4fa3c49b89cbd107f2951d275",
            "a2c39ea3584b4ab396f4e6251797160c",
            "f94e7f8deab64ce8a12ff00b20b8edcf",
            "e70acc135e064204bf13bd04b89508e9",
            "40515c0214114c2684e5d0aac97095b7",
            "b7c4f167e4104f7faae4038bfe2fc8fa",
            "6d89e86e95c14ca18bd37096f7d6050d",
            "06aaca97ce79453a9ec01a13c03419bf"
          ]
        },
        "id": "j_GWKug12Wel",
        "outputId": "bd3f3a6a-2d47-4058-d6c7-9e996563a258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec68c59b7ec4453eaef7a938a082a7ce"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenized_dataset[\"train\"][0][\"input_ids\"]))  # Uzunluk hep 256 olmalı\n",
        "print(len(tokenized_dataset[\"train\"][0][\"labels\"]))  # Uzunluk hep 256 olmalı\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvsI5_o8-0MJ",
        "outputId": "a9e28f97-df9d-433a-97aa-7892bfcce04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256\n",
            "256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"tokenized_dataset.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenized_dataset, f)\n"
      ],
      "metadata": {
        "id": "UO6BZ6IDDdq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# LoRA yapılandırmasını oluştur\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]\n",
        ")\n",
        "\n",
        "# Modeli yükleyelim\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, token=True)\n",
        "\n",
        "# LoRA adaptörünü ekleyelim\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Modeli eğitime hazır hale getir\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# **TÜM PARAMETRELERİ EĞİTİME AÇ**\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# GPU kontrolü\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4G2G-Dy1-xk",
        "outputId": "a84830e9-94bd-48eb-8553-20fa31546505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModel(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Qwen2ForCausalLM(\n",
              "      (model): Qwen2Model(\n",
              "        (embed_tokens): Embedding(151936, 1536)\n",
              "        (layers): ModuleList(\n",
              "          (0-27): 28 x Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
              "              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
              "              (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "        (rotary_emb): Qwen2RotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yz2utVHCKkS",
        "outputId": "0fcace47-9ee8-4322-fd4c-65c60fcc72f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar 17 08:14:40 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0             50W /  400W |    7309MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset[0][\"input_ids\"]))  # 256 olmalı\n",
        "print(len(train_dataset[0][\"labels\"]))  # 256 olmalı\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSibvlbnCOlb",
        "outputId": "ed9642f1-d9bb-4d09-c767-2a57a025c5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "207\n",
            "34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"tokenized_dataset.pkl\", \"rb\") as f:\n",
        "    tokenized_dataset = pickle.load(f)\n",
        "\n",
        "# **Train ve Test Dataset'ini Tekrar Tanımla!**\n",
        "split_dataset = tokenized_dataset[\"train\"].train_test_split(test_size=0.2)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "test_dataset = split_dataset[\"test\"]\n"
      ],
      "metadata": {
        "id": "iKhoPilFELHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset[0][\"input_ids\"]))  # 256 olmalı\n",
        "print(len(train_dataset[0][\"labels\"]))  # 256 olmalı\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhqySNzlENTv",
        "outputId": "92b13758-4c71-4c27-dbae-c70e48901a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256\n",
            "256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p2WONV5Ey0k",
        "outputId": "f8f4f4e2-dd1c-42a8-9504-94e2af56f3d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModel(\n",
            "  (base_model): LoraModel(\n",
            "    (model): Qwen2ForCausalLM(\n",
            "      (model): Qwen2Model(\n",
            "        (embed_tokens): Embedding(151936, 1536)\n",
            "        (layers): ModuleList(\n",
            "          (0-27): 28 x Qwen2DecoderLayer(\n",
            "            (self_attn): Qwen2Attention(\n",
            "              (q_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
            "              (v_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
            "            )\n",
            "            (mlp): Qwen2MLP(\n",
            "              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
            "              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
            "              (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
            "              (act_fn): SiLU()\n",
            "            )\n",
            "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
            "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
            "          )\n",
            "        )\n",
            "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
            "        (rotary_emb): Qwen2RotaryEmbedding()\n",
            "      )\n",
            "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./aya_finetuned\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    push_to_hub=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaRDLZq_3QBs",
        "outputId": "cbf3c098-8288-4e09-df79-99a7dad1f9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n"
      ],
      "metadata": {
        "id": "Scqr_zjY5Ds_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6HzaBTTFfDJ",
        "outputId": "a4ea6e15-9cd3-4669-8f08-8240aaee4be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_model.model.model.embed_tokens.weight\n",
            "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.0.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.0.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.0.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.0.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.0.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.0.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.0.mlp.up_proj.weight\n",
            "base_model.model.model.layers.0.mlp.down_proj.weight\n",
            "base_model.model.model.layers.0.input_layernorm.weight\n",
            "base_model.model.model.layers.0.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.1.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.1.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.1.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.1.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.1.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.1.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.1.mlp.up_proj.weight\n",
            "base_model.model.model.layers.1.mlp.down_proj.weight\n",
            "base_model.model.model.layers.1.input_layernorm.weight\n",
            "base_model.model.model.layers.1.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.2.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.2.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.2.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.2.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.2.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.2.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.2.mlp.up_proj.weight\n",
            "base_model.model.model.layers.2.mlp.down_proj.weight\n",
            "base_model.model.model.layers.2.input_layernorm.weight\n",
            "base_model.model.model.layers.2.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.3.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.3.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.3.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.3.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.3.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.3.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.3.mlp.up_proj.weight\n",
            "base_model.model.model.layers.3.mlp.down_proj.weight\n",
            "base_model.model.model.layers.3.input_layernorm.weight\n",
            "base_model.model.model.layers.3.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.4.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.4.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.4.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.4.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.4.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.4.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.4.mlp.up_proj.weight\n",
            "base_model.model.model.layers.4.mlp.down_proj.weight\n",
            "base_model.model.model.layers.4.input_layernorm.weight\n",
            "base_model.model.model.layers.4.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.5.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.5.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.5.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.5.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.5.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.5.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.5.mlp.up_proj.weight\n",
            "base_model.model.model.layers.5.mlp.down_proj.weight\n",
            "base_model.model.model.layers.5.input_layernorm.weight\n",
            "base_model.model.model.layers.5.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.6.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.6.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.6.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.6.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.6.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.6.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.6.mlp.up_proj.weight\n",
            "base_model.model.model.layers.6.mlp.down_proj.weight\n",
            "base_model.model.model.layers.6.input_layernorm.weight\n",
            "base_model.model.model.layers.6.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.7.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.7.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.7.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.7.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.7.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.7.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.7.mlp.up_proj.weight\n",
            "base_model.model.model.layers.7.mlp.down_proj.weight\n",
            "base_model.model.model.layers.7.input_layernorm.weight\n",
            "base_model.model.model.layers.7.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.8.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.8.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.8.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.8.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.8.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.8.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.8.mlp.up_proj.weight\n",
            "base_model.model.model.layers.8.mlp.down_proj.weight\n",
            "base_model.model.model.layers.8.input_layernorm.weight\n",
            "base_model.model.model.layers.8.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.9.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.9.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.9.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.9.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.9.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.9.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.9.mlp.up_proj.weight\n",
            "base_model.model.model.layers.9.mlp.down_proj.weight\n",
            "base_model.model.model.layers.9.input_layernorm.weight\n",
            "base_model.model.model.layers.9.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.10.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.10.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.10.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.10.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.10.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.10.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.10.mlp.up_proj.weight\n",
            "base_model.model.model.layers.10.mlp.down_proj.weight\n",
            "base_model.model.model.layers.10.input_layernorm.weight\n",
            "base_model.model.model.layers.10.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.11.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.11.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.11.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.11.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.11.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.11.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.11.mlp.up_proj.weight\n",
            "base_model.model.model.layers.11.mlp.down_proj.weight\n",
            "base_model.model.model.layers.11.input_layernorm.weight\n",
            "base_model.model.model.layers.11.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.12.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.12.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.12.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.12.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.12.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.12.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.12.mlp.up_proj.weight\n",
            "base_model.model.model.layers.12.mlp.down_proj.weight\n",
            "base_model.model.model.layers.12.input_layernorm.weight\n",
            "base_model.model.model.layers.12.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.13.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.13.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.13.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.13.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.13.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.13.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.13.mlp.up_proj.weight\n",
            "base_model.model.model.layers.13.mlp.down_proj.weight\n",
            "base_model.model.model.layers.13.input_layernorm.weight\n",
            "base_model.model.model.layers.13.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.14.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.14.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.14.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.14.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.14.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.14.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.14.mlp.up_proj.weight\n",
            "base_model.model.model.layers.14.mlp.down_proj.weight\n",
            "base_model.model.model.layers.14.input_layernorm.weight\n",
            "base_model.model.model.layers.14.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.15.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.15.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.15.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.15.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.15.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.15.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.15.mlp.up_proj.weight\n",
            "base_model.model.model.layers.15.mlp.down_proj.weight\n",
            "base_model.model.model.layers.15.input_layernorm.weight\n",
            "base_model.model.model.layers.15.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.16.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.16.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.16.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.16.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.16.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.16.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.16.mlp.up_proj.weight\n",
            "base_model.model.model.layers.16.mlp.down_proj.weight\n",
            "base_model.model.model.layers.16.input_layernorm.weight\n",
            "base_model.model.model.layers.16.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.17.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.17.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.17.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.17.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.17.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.17.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.17.mlp.up_proj.weight\n",
            "base_model.model.model.layers.17.mlp.down_proj.weight\n",
            "base_model.model.model.layers.17.input_layernorm.weight\n",
            "base_model.model.model.layers.17.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.18.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.18.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.18.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.18.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.18.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.18.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.18.mlp.up_proj.weight\n",
            "base_model.model.model.layers.18.mlp.down_proj.weight\n",
            "base_model.model.model.layers.18.input_layernorm.weight\n",
            "base_model.model.model.layers.18.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.19.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.19.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.19.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.19.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.19.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.19.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.19.mlp.up_proj.weight\n",
            "base_model.model.model.layers.19.mlp.down_proj.weight\n",
            "base_model.model.model.layers.19.input_layernorm.weight\n",
            "base_model.model.model.layers.19.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.20.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.20.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.20.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.20.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.20.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.20.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.20.mlp.up_proj.weight\n",
            "base_model.model.model.layers.20.mlp.down_proj.weight\n",
            "base_model.model.model.layers.20.input_layernorm.weight\n",
            "base_model.model.model.layers.20.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.21.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.21.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.21.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.21.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.21.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.21.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.21.mlp.up_proj.weight\n",
            "base_model.model.model.layers.21.mlp.down_proj.weight\n",
            "base_model.model.model.layers.21.input_layernorm.weight\n",
            "base_model.model.model.layers.21.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.22.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.22.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.22.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.22.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.22.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.22.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.22.mlp.up_proj.weight\n",
            "base_model.model.model.layers.22.mlp.down_proj.weight\n",
            "base_model.model.model.layers.22.input_layernorm.weight\n",
            "base_model.model.model.layers.22.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.23.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.23.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.23.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.23.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.23.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.23.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.23.mlp.up_proj.weight\n",
            "base_model.model.model.layers.23.mlp.down_proj.weight\n",
            "base_model.model.model.layers.23.input_layernorm.weight\n",
            "base_model.model.model.layers.23.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.24.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.24.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.24.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.24.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.24.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.24.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.24.mlp.up_proj.weight\n",
            "base_model.model.model.layers.24.mlp.down_proj.weight\n",
            "base_model.model.model.layers.24.input_layernorm.weight\n",
            "base_model.model.model.layers.24.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.25.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.25.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.25.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.25.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.25.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.25.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.25.mlp.up_proj.weight\n",
            "base_model.model.model.layers.25.mlp.down_proj.weight\n",
            "base_model.model.model.layers.25.input_layernorm.weight\n",
            "base_model.model.model.layers.25.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.26.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.26.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.26.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.26.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.26.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.26.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.26.mlp.up_proj.weight\n",
            "base_model.model.model.layers.26.mlp.down_proj.weight\n",
            "base_model.model.model.layers.26.input_layernorm.weight\n",
            "base_model.model.model.layers.26.post_attention_layernorm.weight\n",
            "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight\n",
            "base_model.model.model.layers.27.self_attn.q_proj.base_layer.bias\n",
            "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.27.self_attn.k_proj.weight\n",
            "base_model.model.model.layers.27.self_attn.k_proj.bias\n",
            "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight\n",
            "base_model.model.model.layers.27.self_attn.v_proj.base_layer.bias\n",
            "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.27.self_attn.o_proj.weight\n",
            "base_model.model.model.layers.27.mlp.gate_proj.weight\n",
            "base_model.model.model.layers.27.mlp.up_proj.weight\n",
            "base_model.model.model.layers.27.mlp.down_proj.weight\n",
            "base_model.model.model.layers.27.input_layernorm.weight\n",
            "base_model.model.model.layers.27.post_attention_layernorm.weight\n",
            "base_model.model.model.norm.weight\n",
            "base_model.model.lm_head.weight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Toplam Parametre: {total_params / 1e9:.2f}B\")  # Milyar cinsinden göster\n",
        "print(f\"Eğitilebilir Parametre: {trainable_params / 1e6:.2f}M\")  # Milyon cinsinden göster\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWTeVA9eIA4N",
        "outputId": "497ec66a-d931-41da-9fd4-38ff09ce635f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toplam Parametre: 1.78B\n",
            "Eğitilebilir Parametre: 1778.18M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "HLi0xv905YxQ",
        "outputId": "74a6c0f5-dbdb-427d-948e-b80325f538d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7476' max='7476' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7476/7476 1:02:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.313600</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.286500</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.142100</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=7476, training_loss=0.2717528716965497, metrics={'train_runtime': 3753.3509, 'train_samples_per_second': 7.967, 'train_steps_per_second': 1.992, 'total_flos': 7.095677035216896e+16, 'train_loss': 0.2717528716965497, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"trained_model\")\n",
        "tokenizer.save_pretrained(\"trained_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzzqZd0PrC--",
        "outputId": "a1be599d-d58c-4082-b3bf-d110a016c53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('trained_model/tokenizer_config.json',\n",
              " 'trained_model/special_tokens_map.json',\n",
              " 'trained_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwOxEBM8rMO-",
        "outputId": "66df5c5e-51c8-4e71-c95f-59790b37aaa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModel(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Qwen2ForCausalLM(\n",
              "      (model): Qwen2Model(\n",
              "        (embed_tokens): Embedding(151936, 1536)\n",
              "        (layers): ModuleList(\n",
              "          (0-27): 28 x Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
              "              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
              "              (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "        (rotary_emb): Qwen2RotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)\n",
        "print(dataset['train'][10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdnJGhatredJ",
        "outputId": "dfc3bf1a-a77d-48a3-9fe9-ce81dcb09b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_text', 'target_text'],\n",
            "        num_rows: 12460\n",
            "    })\n",
            "})\n",
            "{'input_text': \"#Person1#: Could you do me a favor?\\n#Person2#: Sure. What is it?\\n#Person1#: Could you run over to the store? We need a few things.\\n#Person2#: All right. What do you want me to get?\\n#Person1#: Well, could you pick up some sugar?\\n#Person2#: Okay. How much?\\n#Person1#: A small bag. I guess we also need a few oranges.\\n#Person2#: How many?\\n#Person1#: Oh, let's see. . . About six.\\n#Person2#: Anything else?\\n#Person1#: Yes. We're out of milk.\\n#Person2#: Okay. How much do you want me to get? A gallon?\\n#Person1#: No. I think a half gallon will be enough.\\n#Person2#: Is that all?\\n#Person1#: I think so. Have you got all that?\\n#Person2#: Yes. That's small bag of sugar, four oranges, and a half gallon of milk.\\n#Person1#: Do you have enough money?\\n#Person2#: I think so.\\n#Person1#: Thanks very much. I appreciate it.\", 'target_text': '#Person1# asks #Person2# to do a favor. #Person2# agrees and helps buy a small bag of sugar, six oranges, and a half-gallon of milk.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_results = []\n",
        "\n",
        "for i, test_sample in enumerate(test_dataset):\n",
        "    input_text = tokenizer.decode(test_sample[\"input_ids\"], skip_special_tokens=True)\n",
        "\n",
        "    formatted_input = f\"Can you summarize this conversation briefly?\\n\\n{input_text}\"\n",
        "\n",
        "\n",
        "    input_ids = tokenizer(formatted_input, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_tokens = model.generate(input_ids, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
        "\n",
        "    output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    test_results.append({\"input\": input_text, \"output\": output_text})\n",
        "\n",
        "    print(f\"🔹 **Test {i+1}:**\")\n",
        "    print(\"🔹 **Input:**\", input_text)\n",
        "    print(\"🔹 **Model Output:**\", output_text)\n",
        "    print(\"=\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PQc1NiwtrZR-",
        "outputId": "cc6ae78c-3214-43d1-c574-87a9398575a3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 1:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: What would you say if I told you I was going to quit school?\n",
            "#Person2#: I'd say, think twice about it. Well, you are not going to quit school, are you?\n",
            "#Person1#: I don't know. I failed my exam.\n",
            "#Person2#: What did you get?\n",
            "#Person1#: A B plus.\n",
            "#Person2#: That's not bad.\n",
            "#Person1#: But I should have aced it. I mean I finished the paper so quickly, thinking it's a piece of cake. But when I knew my result, I was like what? That can't be true!\n",
            "#Person2#: Listen, John. I understand that you are such an excellent student and I know you must have lots of stress, but you really want to give up?\n",
            "#Person1#: No, I don't. I just don't know how to handle this. This is my first time, you know.\n",
            "#Person2#: I know. Don't worry. Do you know what you should do?\n",
            "#Person1#: What?\n",
            "#Person2#: You need to blow off some steam. The world is not on your shoulder.\n",
            "#Person1#: How\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: What would you say if I told you I was going to quit school?\n",
            "#Person2#: I'd say, think twice about it. Well, you are not going to quit school, are you?\n",
            "#Person1#: I don't know. I failed my exam.\n",
            "#Person2#: What did you get?\n",
            "#Person1#: A B plus.\n",
            "#Person2#: That's not bad.\n",
            "#Person1#: But I should have aced it. I mean I finished the paper so quickly, thinking it's a piece of cake. But when I knew my result, I was like what? That can't be true!\n",
            "#Person2#: Listen, John. I understand that you are such an excellent student and I know you must have lots of stress, but you really want to give up?\n",
            "#Person1#: No, I don't. I just don't know how to handle this. This is my first time, you know.\n",
            "#Person2#: I know. Don't worry. Do you know what you should do?\n",
            "#Person1#: What?\n",
            "#Person2#: You need to blow off some steam. The world is not on your shoulder.\n",
            "#Person1#: How can I help you?\n",
            "#Person2#: Take a deep breath. Take a piece of paper and write down what you think you should do next.\n",
            "#Person1#: I don't know.\n",
            "#Person2#: Write down your thoughts and I'll help you.\n",
            "#Person1#: Write down what you think you should do next.\n",
            "#Person2#: Write down your thoughts and I'll help you.\n",
            "#Person1#: Write down your thoughts and I'll help you.\n",
            "#Person1#: Write\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 2:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: What is the difference between a lesson and a lecture?\n",
            "#Person2#: Well, they are both ways of imparting knowledge, but the main difference is that you participate in a lesson whereas you just listen to a lecture. A lecture is generally given to a much larger group.<｜Assistant｜>#Person2# tells #Person1# the difference between a lesson and a lecture.<｜Assistant｜><think>\n",
            "\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: What is the difference between a lesson and a lecture?\n",
            "#Person2#: Well, they are both ways of imparting knowledge, but the main difference is that you participate in a lesson whereas you just listen to a lecture. A lecture is generally given to a much larger group.<｜Assistant｜>#Person2# tells #Person1# the difference between a lesson and a lecture.<｜Assistant｜><think>\n",
            "Okay, so I need to summarize this conversation briefly. Let me read through it again to make sure I understand what's happening.\n",
            "\n",
            "Person1 asks, \"What is the difference between a lesson and a lecture?\" Person2 responds, saying that both are ways of imparting knowledge but that a lesson allows participation while a lecture is more about listening to a larger group. \n",
            "\n",
            "Hmm, so the main points are about what lesson and lecture mean in this context. I think I should explain that lessons are interactive\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 3:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: May, is this the Hall of Ancient China?\n",
            "#Person2#: Yes. Look at these historical relics here, amazing!\n",
            "#Person1#: Why are they all in glass boxes?\n",
            "#Person2#: For protection. Some relics will turn to dust if exposed to air.\n",
            "#Person1#: So there isn't air in the glass boxes?\n",
            "#Person2#: No, there isn't. They are all vacuumed.\n",
            "#Person1#: I wonder how old these things are, thousands of years?\n",
            "#Person2#: Yeah, they all come from a very ancient time.\n",
            "#Person1#: Hey, look at the three-leg cup. I've seen it on TV.\n",
            "#Person2#: The bronze cup was made 4, 000 years ago. It's priceless!\n",
            "#Person1#: Yeah, it has no value at all. Who will use this cup today?\n",
            "#Person2#: Danny, priceless means so valuable that you can't put a price on it.\n",
            "#Person1#: Oh, it does?<｜Assistant｜>May and Danny talk about the historical relics in the Hall of Ancient China.<｜Assistant｜><think>\n",
            "\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: May, is this the Hall of Ancient China?\n",
            "#Person2#: Yes. Look at these historical relics here, amazing!\n",
            "#Person1#: Why are they all in glass boxes?\n",
            "#Person2#: For protection. Some relics will turn to dust if exposed to air.\n",
            "#Person1#: So there isn't air in the glass boxes?\n",
            "#Person2#: No, there isn't. They are all vacuumed.\n",
            "#Person1#: I wonder how old these things are, thousands of years?\n",
            "#Person2#: Yeah, they all come from a very ancient time.\n",
            "#Person1#: Hey, look at the three-leg cup. I've seen it on TV.\n",
            "#Person2#: The bronze cup was made 4, 000 years ago. It's priceless!\n",
            "#Person1#: Yeah, it has no value at all. Who will use this cup today?\n",
            "#Person2#: Danny, priceless means so valuable that you can't put a price on it.\n",
            "#Person1#: Oh, it does?<｜Assistant｜>May and Danny talk about the historical relics in the Hall of Ancient China.<｜Assistant｜><think>\n",
            "Okay, so I need to summarize this conversation briefly. Let me read through it carefully.\n",
            "\n",
            "Person 1 asks if it's the Hall of Ancient China, and Person 2 confirms. They mention looking at historical relics, which is a good start. Then Person 1 asks why they're in glass boxes, Person 2 explains that some relics might turn to dust, so they're protected. They mention that the boxes are vacuumed, so no air inside. Person 1 is curious about the\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 4:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Excuse me may I know the name of your corporation?\n",
            "#Person2#: China National Textiles Import and Export Corporation\n",
            "#Person1#: We found your samples very attractive. We're interested in buying your garments if your prices are reasonable\n",
            "#Person2#: Our products are of good quality. Since the prices of textiles have gone up in the world market we've had to increase them on some items by about ten percent. If you place a large order it is possible for us to cut down the prices by five percent\n",
            "#Person1#: Can I have a reduction of seven percent?\n",
            "#Person2#: It depends on how much you order\n",
            "#Person1#: Can I use the FAS terms?\n",
            "#Person2#: I'd rather use the CIF terms. You're welcome to come to talk about it in detail with your general manager and sign a contract.<｜Assistant｜>#Person1# tells #Person2# they're interested in buying #Person2#'s garments at reasonable prices. #Person2# says the price depends on the order and suggests talking to the general manager.<｜Assistant｜><think>\n",
            "\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Excuse me may I know the name of your corporation?\n",
            "#Person2#: China National Textiles Import and Export Corporation\n",
            "#Person1#: We found your samples very attractive. We're interested in buying your garments if your prices are reasonable\n",
            "#Person2#: Our products are of good quality. Since the prices of textiles have gone up in the world market we've had to increase them on some items by about ten percent. If you place a large order it is possible for us to cut down the prices by five percent\n",
            "#Person1#: Can I have a reduction of seven percent?\n",
            "#Person2#: It depends on how much you order\n",
            "#Person1#: Can I use the FAS terms?\n",
            "#Person2#: I'd rather use the CIF terms. You're welcome to come to talk about it in detail with your general manager and sign a contract.<｜Assistant｜>#Person1# tells #Person2# they're interested in buying #Person2#'s garments at reasonable prices. #Person2# says the price depends on the order and suggests talking to the general manager.<｜Assistant｜><think>\n",
            "Okay, so I need to summarize this conversation briefly. Let me read through it again to understand the key points.\n",
            "\n",
            "Person1 is asking about the name of their corporation. Person2 tells them it's China National Textiles Import and Export Corporation. Then, Person1 mentions that they found the samples attractive and are interested in buying the garments. Person2 explains that their products are good quality but prices have gone up in the world market, so they've increased some items by ten percent. \n",
            "\n",
            "Person1\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 5:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: What kind of personality do you think you have?\n",
            "#Person2#: I am quite active and energetic. I approach things enthusiastically and I don't like leaving things half done.\n",
            "#Person1#: Do you think you are introverted or extroverted?\n",
            "#Person2#: I am quite outgoing, I think. I enjoy mixing and doing things with others.\n",
            "#Person1#: What do you think is the most important thing for you to be happy?\n",
            "#Person2#: I maintain that the most important thing is having good friends. A person can't live alt by himself, I think. A friend in need is a friend indeed. So the more really close friends I have, the better.\n",
            "#Person1#: What kind of people do you like to work with?\n",
            "#Person2#: People who are honest, dedicate to their work and have integrity.\n",
            "#Person1#: What kind of people do you find difficult to work with?\n",
            "#Person2#: Slacker and those who violate working procedures and ignoring deadlines.<｜Assistant｜>#Person1# asks #Person2# about #Person2#'s personality. #Person2# says #Person2# is active and outgoing. #Person2# values friends\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: What kind of personality do you think you have?\n",
            "#Person2#: I am quite active and energetic. I approach things enthusiastically and I don't like leaving things half done.\n",
            "#Person1#: Do you think you are introverted or extroverted?\n",
            "#Person2#: I am quite outgoing, I think. I enjoy mixing and doing things with others.\n",
            "#Person1#: What do you think is the most important thing for you to be happy?\n",
            "#Person2#: I maintain that the most important thing is having good friends. A person can't live alt by himself, I think. A friend in need is a friend indeed. So the more really close friends I have, the better.\n",
            "#Person1#: What kind of people do you like to work with?\n",
            "#Person2#: People who are honest, dedicate to their work and have integrity.\n",
            "#Person1#: What kind of people do you find difficult to work with?\n",
            "#Person2#: Slacker and those who violate working procedures and ignoring deadlines.<｜Assistant｜>#Person1# asks #Person2# about #Person2#'s personality. #Person2# says #Person2# is active and outgoing. #Person2# values friends for happiness. #Person2# likes people who are honest and dedicated, but finds slacker and dishonest people difficult to work with.\n",
            "</think>\n",
            "\n",
            "**Summary:**\n",
            "\n",
            "- **Person 1**: Inquire about #Person2#'s personality.\n",
            "- **Person 2**: Confirms #Person2#'s activity and eagerness, dislikes unfinished tasks, identifies themselves as outgoing, and enjoys interactions with others.\n",
            "- **Person 2** emphasizes the importance of friends for happiness and highlights the need\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 6:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Joe, can you squeeze me in sometime today?\n",
            "#Person2#: That's a big order, Jane. I am really swamped.\n",
            "#Person1#: I know what you're saying, but I have to go over the books with you before I go see our tax guy.\n",
            "#Person2#: Right. Okay, let me see what I can do. How about 1:30 right after my lunch meeting?<｜Assistant｜>Jane asks Joe to find some time today to go over the books.<｜Assistant｜><think>\n",
            "\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Joe, can you squeeze me in sometime today?\n",
            "#Person2#: That's a big order, Jane. I am really swamped.\n",
            "#Person1#: I know what you're saying, but I have to go over the books with you before I go see our tax guy.\n",
            "#Person2#: Right. Okay, let me see what I can do. How about 1:30 right after my lunch meeting?<｜Assistant｜>Jane asks Joe to find some time today to go over the books.<｜Assistant｜><think>\n",
            "Okay, so I need to summarize this conversation between Joe and Jane. Let me read through it again to make sure I get all the key points.\n",
            "\n",
            "Joe starts by asking Jane to squeeze him in sometime today. Jane responds with a big order, so she's swamped. Joe then says he knows what Jane is saying but needs to go over the books with her before seeing the tax guy. Jane agrees to meet at 1:30 after her lunch meeting.\n",
            "\n",
            "So, the main points are\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 7:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Now I know why I split up with Mike. We found we were simply not good for each other. \n",
            "#Person2#: In what ways? \n",
            "#Person1#: Well, he is a typical Sagittarius guy, while I am a Cancer. We aren't really compatible \n",
            "#Person2#: Ha-ha, so you believe in astrology? \n",
            "#Person1#: What's strange about that! As a person born under the sign of Cancer, I am home-loving and wish for a peaceful family life. But according to astrology, Sagittarius guys are too adventurous and risk-taking. They seldom think of leading a settled and peaceful life. \n",
            "#Person2#: Is that so? I'm afraid it is too narrow-minded to judge people using astrology. It's all stereotypes! \n",
            "#Person1#: But in my case, the fact matches the theory. Mike is humorous, energetic, always as fresh as a daisy, but probably too ambitious. It frightens me! \n",
            "#Person2#: But as far as I remember, you two caught on like a house on fire when you first met. \n",
            "#Person1#: Exactly. But later on, he cares more about his\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Now I know why I split up with Mike. We found we were simply not good for each other. \n",
            "#Person2#: In what ways? \n",
            "#Person1#: Well, he is a typical Sagittarius guy, while I am a Cancer. We aren't really compatible \n",
            "#Person2#: Ha-ha, so you believe in astrology? \n",
            "#Person1#: What's strange about that! As a person born under the sign of Cancer, I am home-loving and wish for a peaceful family life. But according to astrology, Sagittarius guys are too adventurous and risk-taking. They seldom think of leading a settled and peaceful life. \n",
            "#Person2#: Is that so? I'm afraid it is too narrow-minded to judge people using astrology. It's all stereotypes! \n",
            "#Person1#: But in my case, the fact matches the theory. Mike is humorous, energetic, always as fresh as a daisy, but probably too ambitious. It frightens me! \n",
            "#Person2#: But as far as I remember, you two caught on like a house on fire when you first met. \n",
            "#Person1#: Exactly. But later on, he cares more about his friends than his family. \n",
            "#Person2#: Well, if I were you, I would have... well, I don't know. Maybe I would have been more careful. But as it is, we don't have much time to think about it. \n",
            "#Person1#: So, I guess we're moving on. We won't dwell on it anymore. \n",
            "#Person2#: I understand. I don't have to. It's not something I'm here to dictate. I just\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 8:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: The time has come to say goodbye.\n",
            "#Person2#: So soon. It seems as if you've just got here.\n",
            "#Person1#: I feel that way, too, but they say all good things must come to an end.\n",
            "#Person2#: It certainly has been a pleasure seeing you again and renewing old memories.\n",
            "#Person1#: I've had a nice time and I really want to thank you for spending so much time showing me the sights.\n",
            "#Person2#: Oh. It was fun for me, too. It gave me the chance to get away from my everyday work and do something a little bit different.\n",
            "#Person1#: You'll be out to see us next year, then, as you promised?\n",
            "#Person2#: Oh, yes. That's our present plan unless something bad comes up. We should be there sometime in early September.\n",
            "#Person1#: We'll be expecting you.<｜Assistant｜>#Person1#'s leaving and thanks #Person2# for showing #Person1# the sights. #Person2# promises to visit #Person1# next year.<｜Assistant｜><think>\n",
            "\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: The time has come to say goodbye.\n",
            "#Person2#: So soon. It seems as if you've just got here.\n",
            "#Person1#: I feel that way, too, but they say all good things must come to an end.\n",
            "#Person2#: It certainly has been a pleasure seeing you again and renewing old memories.\n",
            "#Person1#: I've had a nice time and I really want to thank you for spending so much time showing me the sights.\n",
            "#Person2#: Oh. It was fun for me, too. It gave me the chance to get away from my everyday work and do something a little bit different.\n",
            "#Person1#: You'll be out to see us next year, then, as you promised?\n",
            "#Person2#: Oh, yes. That's our present plan unless something bad comes up. We should be there sometime in early September.\n",
            "#Person1#: We'll be expecting you.<｜Assistant｜>#Person1#'s leaving and thanks #Person2# for showing #Person1# the sights. #Person2# promises to visit #Person1# next year.<｜Assistant｜><think>\n",
            "Okay, so I need to summarize this conversation between Person1 and Person2. Let me read through the conversation again to get a clear picture.\n",
            "\n",
            "Person1 starts by saying, \"The time has come to say goodbye.\" Then Person2 responds, \"So soon. It seems as if you've just got here.\" Person1 agrees and adds, \"I feel that way, too, but they say all good things must come to an end.\" Person2 responds with, \"It certainly has been a\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 9:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Hey, Mike. Can I ask you something?\n",
            "#Person2#: Sure, what's up?\n",
            "#Person1#: I'm going to gamble on the world cup tonight. Who do you think will win?\n",
            "#Person2#: Good question. I have to think about it.\n",
            "#Person1#: My husband is betting on France.\n",
            "#Person2#: France? Is he crazy?\n",
            "#Person1#: What do you mean?\n",
            "#Person2#: Well, how do I put this? France is a terrible team.<｜Assistant｜>#Person1# 's husband is betting on France on the world cup, but #Person2# thinks France is terrible.<｜Assistant｜><think>\n",
            "\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Hey, Mike. Can I ask you something?\n",
            "#Person2#: Sure, what's up?\n",
            "#Person1#: I'm going to gamble on the world cup tonight. Who do you think will win?\n",
            "#Person2#: Good question. I have to think about it.\n",
            "#Person1#: My husband is betting on France.\n",
            "#Person2#: France? Is he crazy?\n",
            "#Person1#: What do you mean?\n",
            "#Person2#: Well, how do I put this? France is a terrible team.<｜Assistant｜>#Person1# 's husband is betting on France on the world cup, but #Person2# thinks France is terrible.<｜Assistant｜><think>\n",
            "Okay, so I'm trying to figure out how to summarize this conversation between Person1 and Person2. Let me read through the conversation again to make sure I get the main points.\n",
            "\n",
            "Person1 starts by saying, \"Hey, Mike. Can I ask you something?\" That seems like he's introducing himself and perhaps wants some information. Then Person2 responds, \"Sure, what's up?\" That's a friendly greeting, so maybe they're just hanging out or talking about something else.\n",
            "\n",
            "Next,\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 10:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: You won't believe who's been elected to do overtime on the Baker account! Me! I've already logged in 20 hours of overtime! \n",
            "#Person2#: Wow! Why so much? I thought they were getting you an assistant. \n",
            "#Person1#: They were supposed to, but so far nobody's turned up, and I'm left on my own to do the work. This is the first break I've had all day. \n",
            "#Person2#: They're really running you into the ground. Why don't you ask for some time off? You could take a long weekend and go away somewhere. <｜Assistant｜>#Person1#'s been elected to do overtime without an assistant. #Person2# suggests #Person1# ask for some time off.<｜Assistant｜><think>\n",
            "\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: You won't believe who's been elected to do overtime on the Baker account! Me! I've already logged in 20 hours of overtime! \n",
            "#Person2#: Wow! Why so much? I thought they were getting you an assistant. \n",
            "#Person1#: They were supposed to, but so far nobody's turned up, and I'm left on my own to do the work. This is the first break I've had all day. \n",
            "#Person2#: They're really running you into the ground. Why don't you ask for some time off? You could take a long weekend and go away somewhere. <｜Assistant｜>#Person1#'s been elected to do overtime without an assistant. #Person2# suggests #Person1# ask for some time off.<｜Assistant｜><think>\n",
            "Okay, so I need to summarize this conversation between Person1 and Person2. Let me read through the text again to make sure I understand what's going on.\n",
            "\n",
            "Person1 says they won't believe who's been elected to do overtime on the Baker account. They've logged in for 20 hours of overtime. Person2 responds, saying Wow! Why so much? I thought they were getting you an assistant. Then Person1 clarifies that they were supposed to, but so far nobody's\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 11:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Are you going to Helen's birthday party on Friday evening?\n",
            "#Person2#: I wouldn't miss it for the world! It's sure to be fun. She's invited a lot of people. Do you think everyone will be able to get into her house?\n",
            "#Person1#: If everyone turned up, it would be a squeeze, but a few people said that they couldn't go, so I think it should be ok?\n",
            "#Person2#: Are you taking anything?\n",
            "#Person1#: I'Ve got her a birthday present and I'll take a bottle fo wine too.\n",
            "#Person2#: That's a good idea. She told me that she had bought plenty of food and snacks. I think it's going to be a noisy party. I hope her neighbours don't mind too much.\n",
            "#Person1#: Helen gets on very well with her neighbours. I wouldn't be surprised if they went to the party too.\n",
            "#Person2#: I'm ready looking forward to it. This party is going to be a blast!\n",
            "#Person1#: Well, don't be late. I'll see you on Friday at Helen's.<｜Assistant｜>#Person1# and #Person2# are going\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Are you going to Helen's birthday party on Friday evening?\n",
            "#Person2#: I wouldn't miss it for the world! It's sure to be fun. She's invited a lot of people. Do you think everyone will be able to get into her house?\n",
            "#Person1#: If everyone turned up, it would be a squeeze, but a few people said that they couldn't go, so I think it should be ok?\n",
            "#Person2#: Are you taking anything?\n",
            "#Person1#: I'Ve got her a birthday present and I'll take a bottle fo wine too.\n",
            "#Person2#: That's a good idea. She told me that she had bought plenty of food and snacks. I think it's going to be a noisy party. I hope her neighbours don't mind too much.\n",
            "#Person1#: Helen gets on very well with her neighbours. I wouldn't be surprised if they went to the party too.\n",
            "#Person2#: I'm ready looking forward to it. This party is going to be a blast!\n",
            "#Person1#: Well, don't be late. I'll see you on Friday at Helen's.<｜Assistant｜>#Person1# and #Person2# are going to Helen's birthday party on Friday evening. #Person2# is excited about the fun and lots of people invited. They'll be surprised if everyone shows up, but there might be some people who can't make it. #Person1# suggests that if enough people show up, it will be a good experience. They'll take a bottle of wine and the birthday present they've received. #Person2# is looking forward to the party and is excited about Helen's friendliness and neighbors.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 12:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Hey John, did you register for classes yet?\n",
            "#Person2#: I register this Friday.\n",
            "#Person1#: What classes do you plan on taking?\n",
            "#Person2#: I really want to take the communication class, but I don't know if it will be available.\n",
            "#Person1#: Is that class really that popular?\n",
            "#Person2#: Yeah. I tried to get in last semester, but it was full by the time I registered.\n",
            "#Person1#: What other classes are you going to take?\n",
            "#Person2#: I still need to take English 201, but I really don't like writing.\n",
            "#Person1#: I took that class already. There is a lot of writing, but it's not that bad.\n",
            "#Person2#: Oh really? Who was the instructor? There are like 4 different instructors to choose from.\n",
            "#Person1#: I had Professor Mahoney.\n",
            "#Person2#: Is he an easy grader?\n",
            "#Person1#: I'm not sure if he is or not, but I thought he was definitely fair.\n",
            "#Person2#: Do you mind if I ask you what you got?\n",
            "#Person1#: Not at all. I got a\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Hey John, did you register for classes yet?\n",
            "#Person2#: I register this Friday.\n",
            "#Person1#: What classes do you plan on taking?\n",
            "#Person2#: I really want to take the communication class, but I don't know if it will be available.\n",
            "#Person1#: Is that class really that popular?\n",
            "#Person2#: Yeah. I tried to get in last semester, but it was full by the time I registered.\n",
            "#Person1#: What other classes are you going to take?\n",
            "#Person2#: I still need to take English 201, but I really don't like writing.\n",
            "#Person1#: I took that class already. There is a lot of writing, but it's not that bad.\n",
            "#Person2#: Oh really? Who was the instructor? There are like 4 different instructors to choose from.\n",
            "#Person1#: I had Professor Mahoney.\n",
            "#Person2#: Is he an easy grader?\n",
            "#Person1#: I'm not sure if he is or not, but I thought he was definitely fair.\n",
            "#Person2#: Do you mind if I ask you what you got?\n",
            "#Person1#: Not at all. I got a D.\n",
            "#Person2#: That's not good. I was hoping to get an A.\n",
            "#Person1#: I was actually thinking of taking a different class.\n",
            "#Person2#: Maybe a different subject.\n",
            "#Person1#: Maybe a foreign language?\n",
            "#Person2#: No, I don't like that.\n",
            "#Person1#: I think I'll take a course in foreign relations.\n",
            "#Person2#: Oh, I can see why. It's probably going to be hard.\n",
            "#Person1\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 13:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Have we sold out all the newspapers for today? \n",
            "#Person2#: Yes. What a good job we have done! \n",
            "#Person1#: Why are so many extra gifts left here? \n",
            "#Person2#: Many customers didn't want them. \n",
            "#Person1#: How about giving them to that old man? \n",
            "#Person2#: Good idea. \n",
            "#Person1#: By the way, do you think it's a good idea to give free gifts to attract customers? \n",
            "#Person2#: I'm not sure, but still it did attract a large crowds today. \n",
            "#Person1#: I think it should be more useful to hand out some fliers which is also cheaper. \n",
            "#Person2#: But people can just throw them into the trash can as they turn around. \n",
            "#Person1#: That's true. \n",
            "#Person2#: Anyway, let's just finish out job and get back home. \n",
            "#Person1#: Okay. How tiring the job is! <｜Assistant｜>#Person2# isn't sure whether it is a good idea to give gifts to customers. #Person1# thinks fliers may be more useful and cheaper, but #Person2#\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Have we sold out all the newspapers for today? \n",
            "#Person2#: Yes. What a good job we have done! \n",
            "#Person1#: Why are so many extra gifts left here? \n",
            "#Person2#: Many customers didn't want them. \n",
            "#Person1#: How about giving them to that old man? \n",
            "#Person2#: Good idea. \n",
            "#Person1#: By the way, do you think it's a good idea to give free gifts to attract customers? \n",
            "#Person2#: I'm not sure, but still it did attract a large crowds today. \n",
            "#Person1#: I think it should be more useful to hand out some fliers which is also cheaper. \n",
            "#Person2#: But people can just throw them into the trash can as they turn around. \n",
            "#Person1#: That's true. \n",
            "#Person2#: Anyway, let's just finish out job and get back home. \n",
            "#Person1#: Okay. How tiring the job is! <｜Assistant｜>#Person2# isn't sure whether it is a good idea to give gifts to customers. #Person1# thinks fliers may be more useful and cheaper, but #Person2# is unsure and suggests that just throwing them into trash might be better. The conversation ends with both having a good time working together.\n",
            "</think>\n",
            "\n",
            "The conversation revolves around a group discussing gift distribution and customer reception. Person 2 is uncertain about the effectiveness of giving free gifts and suggests that just throwing fliers into the trash might be more practical. The group successfully navigates the challenge and enjoys their collaborative work.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 14:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: How is everything going with your girlfriend? \n",
            "#Person2#: Didn't I tell you? It's over! \n",
            "#Person1#: Oh, I am sorry to hear that. I did't know that you had split up. What happened? \n",
            "#Person2#: It was a few things. The first thing that happened was that we were supposed to go out for a romantic dinner for our one year anniversary, but she stood me up! \n",
            "#Person1#: Really! Did she tell you why she didn't show up? \n",
            "#Person2#: No, but I ended up finding out later that night when I saw her with another man at a club near my home! \n",
            "#Person1#: What was she thinking? Did you confront her about it when you saw her? \n",
            "#Person2#: I wanted to, but I knew that if I spoke to her, I'd just blow up at her, so I decided to just go home. I called her later that night, but she didn't answer the phone. \n",
            "#Person1#: I can't believe she would do that to you. It's so dishonest---and rude! \n",
            "#Person2#: I know. I\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: How is everything going with your girlfriend? \n",
            "#Person2#: Didn't I tell you? It's over! \n",
            "#Person1#: Oh, I am sorry to hear that. I did't know that you had split up. What happened? \n",
            "#Person2#: It was a few things. The first thing that happened was that we were supposed to go out for a romantic dinner for our one year anniversary, but she stood me up! \n",
            "#Person1#: Really! Did she tell you why she didn't show up? \n",
            "#Person2#: No, but I ended up finding out later that night when I saw her with another man at a club near my home! \n",
            "#Person1#: What was she thinking? Did you confront her about it when you saw her? \n",
            "#Person2#: I wanted to, but I knew that if I spoke to her, I'd just blow up at her, so I decided to just go home. I called her later that night, but she didn't answer the phone. \n",
            "#Person1#: I can't believe she would do that to you. It's so dishonest---and rude! \n",
            "#Person2#: I know. I just don't understand how someone could have acted like that. She must have been so upset. She didn't even think about what she'd done. \n",
            "#Person1#: I'm really sorry to hear that. I know it's going to be tough for you both. I just want to make sure we move forward. \n",
            "#Person2#: I can understand. I just want you to be okay. I just want you to feel good about yourself. I don't want to see this kind\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 15:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Hello.\n",
            "#Person2#: This is Jim Evans from the Market Research Company. May I ask you a few questions?\n",
            "#Person1#: Oh, I suppose.\n",
            "#Person2#: Do you read any newspapers and if so, which ones?\n",
            "#Person1#: I read the New York Times and the Wall Street Journal.\n",
            "#Person2#: About how many hours a week do you read newspapers then?\n",
            "#Person1#: Oh, about 2 or 3.\n",
            "#Person2#: OK. Now, do you read books regularly and if so, what type?\n",
            "#Person1#: Well, novels are my favorite. But to tell you the truth, I haven't read one in quite a while. I'm taking a class at night, so the only books I've been reading lately, our textbooks.\n",
            "#Person2#: And how long do you spend a week reading textbooks?\n",
            "#Person1#: Not enough. I guess 7 or 8 hours.\n",
            "#Person2#: Thanks for your help.\n",
            "#Person1#: You're welcome.<｜Assistant｜>Jim Evans from the Market Research Company calls #Person1# to ask some questions about reading. #Person1# reads newspapers, and textbooks regularly,\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Hello.\n",
            "#Person2#: This is Jim Evans from the Market Research Company. May I ask you a few questions?\n",
            "#Person1#: Oh, I suppose.\n",
            "#Person2#: Do you read any newspapers and if so, which ones?\n",
            "#Person1#: I read the New York Times and the Wall Street Journal.\n",
            "#Person2#: About how many hours a week do you read newspapers then?\n",
            "#Person1#: Oh, about 2 or 3.\n",
            "#Person2#: OK. Now, do you read books regularly and if so, what type?\n",
            "#Person1#: Well, novels are my favorite. But to tell you the truth, I haven't read one in quite a while. I'm taking a class at night, so the only books I've been reading lately, our textbooks.\n",
            "#Person2#: And how long do you spend a week reading textbooks?\n",
            "#Person1#: Not enough. I guess 7 or 8 hours.\n",
            "#Person2#: Thanks for your help.\n",
            "#Person1#: You're welcome.<｜Assistant｜>Jim Evans from the Market Research Company calls #Person1# to ask some questions about reading. #Person1# reads newspapers, and textbooks regularly, but only 7-8 hours a week. #Person2# responds by asking how many hours he reads newspapers and textbooks. #Person1# mentions reading the New York Times and Wall Street Journal, about 2-3 hours a week, and his favorite book is a novel but has been resting on him because he's taking a night class. #Person2# is thankful and confirms his help.\n",
            "</think>\n",
            "\n",
            "Jim Evans from the Market Research Company asks about reading habits. #Person1\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 16:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Don't you ever cheat on me.\n",
            "#Person2#: Why would I do that?\n",
            "#Person1#: Because men like to cheat.\n",
            "#Person2#: Some men do, but not me.\n",
            "#Person1#: I'm watching you.\n",
            "#Person2#: I'm an open book. Watch me all you want.\n",
            "#Person1#: If I catch you, you'll be sorry.\n",
            "#Person2#: You won't catch me, because I love you. I'm not a cheater.\n",
            "#Person1#: I will poke your eyes out.\n",
            "#Person2#: I don't want any other woman.\n",
            "#Person1#: I will chop your toes off, one by one.\n",
            "#Person2#: Honey, please. You're the only woman for me, forever. I swear it.<｜Assistant｜>#Person1# asks #Person2# to never cheat on her. #Person2# assures #Person1# of his loyalty over and over again.<｜Assistant｜><think>\n",
            "\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Don't you ever cheat on me.\n",
            "#Person2#: Why would I do that?\n",
            "#Person1#: Because men like to cheat.\n",
            "#Person2#: Some men do, but not me.\n",
            "#Person1#: I'm watching you.\n",
            "#Person2#: I'm an open book. Watch me all you want.\n",
            "#Person1#: If I catch you, you'll be sorry.\n",
            "#Person2#: You won't catch me, because I love you. I'm not a cheater.\n",
            "#Person1#: I will poke your eyes out.\n",
            "#Person2#: I don't want any other woman.\n",
            "#Person1#: I will chop your toes off, one by one.\n",
            "#Person2#: Honey, please. You're the only woman for me, forever. I swear it.<｜Assistant｜>#Person1# asks #Person2# to never cheat on her. #Person2# assures #Person1# of his loyalty over and over again.<｜Assistant｜><think>\n",
            "Okay, so I'm trying to help someone summarize this conversation. The conversation is between two people, Person1 and Person2. They're talking about something, but I don't know what it is, but I can see the main topics are about cheating or loyalty.\n",
            "\n",
            "First, I'll read through the conversation again to get the gist. Person1 starts by saying, \"Don't you ever cheat on me.\" That's pretty direct. Then Person2 responds, \"Why would I do that?\"\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 17:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: I want to buy a gift for my father. Will you go with me to choose one for him?\n",
            "#Person2#: Sure. Is it your father's birthday?\n",
            "#Person1#: Don't you know? Tomorrow is Father's Day.\n",
            "#Person2#: Oh! How could I forget such an important holiday?\n",
            "#Person1#: You also need to choose a gift for your father, right?\n",
            "#Person2#: Yes, let's go.<｜Assistant｜>#Person1# and #Person2# shop gifts for their father.<｜Assistant｜><think>\n",
            "\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: I want to buy a gift for my father. Will you go with me to choose one for him?\n",
            "#Person2#: Sure. Is it your father's birthday?\n",
            "#Person1#: Don't you know? Tomorrow is Father's Day.\n",
            "#Person2#: Oh! How could I forget such an important holiday?\n",
            "#Person1#: You also need to choose a gift for your father, right?\n",
            "#Person2#: Yes, let's go.<｜Assistant｜>#Person1# and #Person2# shop gifts for their father.<｜Assistant｜><think>\n",
            "Okay, so I need to summarize this conversation briefly. Let me read through it again.\n",
            "\n",
            "Person1 asks if she can help buy a gift for her father. Person2 says yes, it's his birthday. Person1 then says she knows it's tomorrow, so she'll go. Person2 confirms and also asks if she needs to choose a gift for her father as well.\n",
            "\n",
            "Hmm, so the main points are that both want to help buy a gift for their father on his birthday, and they\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 18:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: What are you doing here?\n",
            "#Person2#: I'm preparing my fishing tools.\n",
            "#Person1#: What for?\n",
            "#Person2#: I'm going to fish with some of my friends.\n",
            "#Person1#: Are you fond of fishing?\n",
            "#Person2#: Yes. I like fishing after school and on Sundays.\n",
            "#Person1#: When did you get such a hobby?\n",
            "#Person2#: Long ago. You know, my father is fond of and very good at fishing. When I was a child, he took me to the riverside to see him fishing. I found fishing very interesting then. Well, what's your hobby?\n",
            "#Person1#: I'm very interested in making home movies. But I haven't got a movie camera.\n",
            "#Person2#: I also like taking photos and I have a Japanese-made camera.\n",
            "#Person1#: I take a lot of photos, too. But I'm more interested in the history of film, so I really enjoy using a film camera. I'll buy one of my own someday.<｜Assistant｜>#Person2# is preparing fishing tools to fish to meet some friends because fishing has been #Person2#'s hobby long ago. #Person1#'\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: What are you doing here?\n",
            "#Person2#: I'm preparing my fishing tools.\n",
            "#Person1#: What for?\n",
            "#Person2#: I'm going to fish with some of my friends.\n",
            "#Person1#: Are you fond of fishing?\n",
            "#Person2#: Yes. I like fishing after school and on Sundays.\n",
            "#Person1#: When did you get such a hobby?\n",
            "#Person2#: Long ago. You know, my father is fond of and very good at fishing. When I was a child, he took me to the riverside to see him fishing. I found fishing very interesting then. Well, what's your hobby?\n",
            "#Person1#: I'm very interested in making home movies. But I haven't got a movie camera.\n",
            "#Person2#: I also like taking photos and I have a Japanese-made camera.\n",
            "#Person1#: I take a lot of photos, too. But I'm more interested in the history of film, so I really enjoy using a film camera. I'll buy one of my own someday.<｜Assistant｜>#Person2# is preparing fishing tools to fish to meet some friends because fishing has been #Person2#'s hobby long ago. #Person1#'s hobby is making home movies with a film camera, and they are interested in the history of film.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 19:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Amelia, could you spare a few minutes?\n",
            "#Person2#: sure. What do you need?\n",
            "#Person1#: well, I wanted to let you know that I've put in my notice.\n",
            "#Person2#: really? Why?\n",
            "#Person1#: it's complicated. But basically it boils down to one thing. This company is downsizing and I can't continue working for a company that may let me go.\n",
            "#Person2#: but surely they won't fire you! You're one of the most experienced managers here!\n",
            "#Person1#: well, to be honest, there's another reason. I've got a better offer.\n",
            "#Person2#: well that's great news! Congratulations! Where will you be located?\n",
            "#Person1#: the head office is in New York, but I'll be dealing with overseas companies and flying to this side of the world from time to time.\n",
            "#Person2#: it'll be sad to see you go, but it sounds like you're found yourself a great opportunity.\n",
            "#Person1#: I have. I feel lucky. I look forward to dealing with overseas companies and at the same time having a chance to use English more.\n",
            "#Person2#: I\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Amelia, could you spare a few minutes?\n",
            "#Person2#: sure. What do you need?\n",
            "#Person1#: well, I wanted to let you know that I've put in my notice.\n",
            "#Person2#: really? Why?\n",
            "#Person1#: it's complicated. But basically it boils down to one thing. This company is downsizing and I can't continue working for a company that may let me go.\n",
            "#Person2#: but surely they won't fire you! You're one of the most experienced managers here!\n",
            "#Person1#: well, to be honest, there's another reason. I've got a better offer.\n",
            "#Person2#: well that's great news! Congratulations! Where will you be located?\n",
            "#Person1#: the head office is in New York, but I'll be dealing with overseas companies and flying to this side of the world from time to time.\n",
            "#Person2#: it'll be sad to see you go, but it sounds like you're found yourself a great opportunity.\n",
            "#Person1#: I have. I feel lucky. I look forward to dealing with overseas companies and at the same time having a chance to use English more.\n",
            "#Person2#: I'll be in London, but I don't know when exactly.\n",
            "#Person1#: I guess I'll contact you later to discuss how to proceed.\n",
            "#Person2#: but seriously, I'm really not sure where I should be.\n",
            "</think>\n",
            "\n",
            "Amelia has been informed that the company is downsizing, and she seeks advice on her next role. She prefers working remotely but is considering moving to London. Amelia is hopeful about the possibility of working remotely and is looking forward to the chance to use English\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 20:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Now Cathy, do you know when the visitors from India are coming?\n",
            "#Person2#: We offer them three choices: the end of March, the middle of April and the beginning of May, and choose the earliest one which is good actually with exams coming up in May.\n",
            "#Person1#: Right. And how many are coming? Did you say about 12?\n",
            "#Person2#: Yes, they said 12 at first, but changed to 10 this morning.\n",
            "#Person1#: Good, we have 8 weeks to prepare, here are my suggestions. On the first day, a welcome party, then they can visit the schools in the district on the second and third days.\n",
            "#Person2#: We've got to remember this group wants to look at how computers are being used in the classroom.\n",
            "#Person1#: Exactly, so I want to ask Mr. Goodman to give them a talk on this on the afternoon of the third day.\n",
            "#Person2#: That will fit in very nicely.\n",
            "#Person1#: And on the last day, they would do some sightseeing, we could take them on the tour of London but many of them may have been there already,\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Now Cathy, do you know when the visitors from India are coming?\n",
            "#Person2#: We offer them three choices: the end of March, the middle of April and the beginning of May, and choose the earliest one which is good actually with exams coming up in May.\n",
            "#Person1#: Right. And how many are coming? Did you say about 12?\n",
            "#Person2#: Yes, they said 12 at first, but changed to 10 this morning.\n",
            "#Person1#: Good, we have 8 weeks to prepare, here are my suggestions. On the first day, a welcome party, then they can visit the schools in the district on the second and third days.\n",
            "#Person2#: We've got to remember this group wants to look at how computers are being used in the classroom.\n",
            "#Person1#: Exactly, so I want to ask Mr. Goodman to give them a talk on this on the afternoon of the third day.\n",
            "#Person2#: That will fit in very nicely.\n",
            "#Person1#: And on the last day, they would do some sightseeing, we could take them on the tour of London but many of them may have been there already, so it's hard to keep up with that.\n",
            "#Person2#: So they need to prioritize the activities.\n",
            "#Person1#: Right, and on the third day, we have three options for the last activity: sightseeing, sightseeing with some history, or just a relaxed day.\n",
            "#Person2#: Yes, they said that, and that will be our fifth and last suggestion.\n",
            "#Person1#: So, we've got eight days, and each day has a specific task, but that\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 21:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Would you like something to drink, Mum?\n",
            "#Person2#: Yes, give me some coffee, please. No milk, no sugar.\n",
            "#Person1#: Certainly.\n",
            "#Person2#: My friend is coming in a minute, and he loves sweet juice. What kind of juice do you recommend?\n",
            "#Person1#: We have apple juice and strawberry juice. They're both sweet. But recommend strawberry juice. It's upon ordinary. And the apple juice was made yesterday, and stored in the refrigerator.<｜Assistant｜>#Person1# prepares coffee for #Person1#'s mother and recommends juice to his mother's friend who is coming later.<｜Assistant｜><think>\n",
            "\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Would you like something to drink, Mum?\n",
            "#Person2#: Yes, give me some coffee, please. No milk, no sugar.\n",
            "#Person1#: Certainly.\n",
            "#Person2#: My friend is coming in a minute, and he loves sweet juice. What kind of juice do you recommend?\n",
            "#Person1#: We have apple juice and strawberry juice. They're both sweet. But recommend strawberry juice. It's upon ordinary. And the apple juice was made yesterday, and stored in the refrigerator.<｜Assistant｜>#Person1# prepares coffee for #Person1#'s mother and recommends juice to his mother's friend who is coming later.<｜Assistant｜><think>\n",
            "Okay, so I need to summarize this conversation briefly. Let me read through it again to make sure I get all the details.\n",
            "\n",
            "Person1 asks if he'd like something to drink, and Person2 agrees, giving him some coffee. Then Person1 mentions that his friend is coming up, and he loves sweet juice. Person2 responds, saying his friend loves sweet juice and recommends either apple or strawberry juice. Person1 suggests strawberry juice because it's more \"upon ordinary\" and apple juice was made\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 22:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: This place is great. I'm surprised there are so many things here.\n",
            "#Person2#: Yes, but it takes a while to find things. It's not organized as carefully as a regular store.\n",
            "#Person1#: What's the difference between an outlet and a regular store?\n",
            "#Person2#: Usually a clothes company will send their overstock to an outlet. The prices are very low. But you may find faulty products here too.\n",
            "#Person1#: Faulty products?\n",
            "#Person2#: Yes. Sometimes the fault is very small ; if you have a needle and thread, you can fix it yourself. So it's a good deal to buy it.\n",
            "#Person1#: Oh, that's easy for me. I think it's really a good deal.\n",
            "#Person2#: Yes. You can save a lot of money in this way.\n",
            "#Person1#: That's great for me. Look, children's clothes! I'd like to buy some winter clothes for my son and daughter.\n",
            "#Person2#: Why not buy the sweater? It only costs 30 Yuan.\n",
            "#Person1#: Yes, I'll take it. Are there any jeans? Xiaohui's jeans are\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: This place is great. I'm surprised there are so many things here.\n",
            "#Person2#: Yes, but it takes a while to find things. It's not organized as carefully as a regular store.\n",
            "#Person1#: What's the difference between an outlet and a regular store?\n",
            "#Person2#: Usually a clothes company will send their overstock to an outlet. The prices are very low. But you may find faulty products here too.\n",
            "#Person1#: Faulty products?\n",
            "#Person2#: Yes. Sometimes the fault is very small ; if you have a needle and thread, you can fix it yourself. So it's a good deal to buy it.\n",
            "#Person1#: Oh, that's easy for me. I think it's really a good deal.\n",
            "#Person2#: Yes. You can save a lot of money in this way.\n",
            "#Person1#: That's great for me. Look, children's clothes! I'd like to buy some winter clothes for my son and daughter.\n",
            "#Person2#: Why not buy the sweater? It only costs 30 Yuan.\n",
            "#Person1#: Yes, I'll take it. Are there any jeans? Xiaohui's jeans are a bit tight.\n",
            "#Person2#: You can buy a size bigger. So it's more affordable. But you can't buy any size that's too big.\n",
            "#Person1#: Xie Yu's jeans are a bit tight. So, Xiaohui's is better.\n",
            "#Person2#: Yes, but maybe it's expensive for you to buy such jeans.\n",
            "#Person1#: What's the issue?\n",
            "#Person2#: Because of the size restriction. So, you'd better buy X\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 23:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Well, if it isn't the teacher's pet!\n",
            "#Person2#: Stop it. Teacher doesn't treat me any different than she does everyone else.\n",
            "#Person1#: You can't prove that by me. I saw the score on your report.\n",
            "#Person2#: Hey, I worked hard on that report and I deserve the grade I got.\n",
            "#Person1#: It just happened to be the highest grade in the class.\n",
            "#Person2#: You're just jealous because you didn't do as well as you thought you did. Admit it.\n",
            "#Person1#: I worked hard too. But she always gives me a lower grade.\n",
            "#Person2#: If you think that she is not fair with your work then you should talk to her in person instead of stewing over it.<｜Assistant｜>#Person2# gets the highest grade. #Person1# thinks the teacher treats #Person2# better. #Person2# thinks #Person1#'s jealous and suggests #Person1# talk to the teacher.<｜Assistant｜><think>\n",
            "\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Well, if it isn't the teacher's pet!\n",
            "#Person2#: Stop it. Teacher doesn't treat me any different than she does everyone else.\n",
            "#Person1#: You can't prove that by me. I saw the score on your report.\n",
            "#Person2#: Hey, I worked hard on that report and I deserve the grade I got.\n",
            "#Person1#: It just happened to be the highest grade in the class.\n",
            "#Person2#: You're just jealous because you didn't do as well as you thought you did. Admit it.\n",
            "#Person1#: I worked hard too. But she always gives me a lower grade.\n",
            "#Person2#: If you think that she is not fair with your work then you should talk to her in person instead of stewing over it.<｜Assistant｜>#Person2# gets the highest grade. #Person1# thinks the teacher treats #Person2# better. #Person2# thinks #Person1#'s jealous and suggests #Person1# talk to the teacher.<｜Assistant｜><think>\n",
            "Okay, so I need to summarize this conversation between Person1 and Person2. Let me read through each line carefully.\n",
            "\n",
            "Person1 starts by saying, \"Well, if it isn't the teacher's pet!\" That's a bit of a play on words. I think it means that the teacher's pet status is something that doesn't matter here, or maybe it's a joke.\n",
            "\n",
            "Then Person2 responds, \"Stop it. Teacher doesn't treat me any different than she does everyone else.\" They\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 24:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: How are you, Kim? I haven't seen you around for a few days. \n",
            "#Person2#: Oh, I was out sick most of last week. I'm still not completely over it. You'd better not get too close! \n",
            "#Person1#: I'm sorry to hear about that. Why don't you join me for a hot cup of tea? It'll be good for you! \n",
            "#Person2#: Sure, why not? I could use a rest. <｜Assistant｜>#Person1# invites Kim to have a cup of tea.<｜Assistant｜><think>\n",
            "\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: How are you, Kim? I haven't seen you around for a few days. \n",
            "#Person2#: Oh, I was out sick most of last week. I'm still not completely over it. You'd better not get too close! \n",
            "#Person1#: I'm sorry to hear about that. Why don't you join me for a hot cup of tea? It'll be good for you! \n",
            "#Person2#: Sure, why not? I could use a rest. <｜Assistant｜>#Person1# invites Kim to have a cup of tea.<｜Assistant｜><think>\n",
            "Okay, so I need to summarize this conversation between Person1 and Person2. Let me read through it again to make sure I understand what's happening.\n",
            "\n",
            "Person1 starts by saying, \"How are you, Kim? I haven't seen you around for a few days.\" That's a casual way of saying they haven't seen Kim in a while, maybe because they've been busy.\n",
            "\n",
            "Then Person2 responds, \"Oh, I was out sick most of last week. I'm still not completely\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 25:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: It's so earlier this morning, Mr. Li! You just came back yesterday, you should have rest much more.\n",
            "#Person2#: I'd like to have rest, but I have many things to do.\n",
            "#Person1#: How about this post-trip?\n",
            "#Person2#: It's very good, and the result isn't too bad.\n",
            "#Person1#: Really?\n",
            "#Person2#: Of course. I will write a report about this post-trip, and then you'd better type it at the moment, OK?\n",
            "#Person1#: OK, it's no problem.\n",
            "#Person2#: ( Afternoon ) Miss Liu. Are you typing my report?\n",
            "#Person1#: Yes, what can I do for you?\n",
            "#Person2#: Nothing. I just tell you take care of it when you type, especially the address and telephone number of those new guests. You should make sure that it's no mistaken.\n",
            "#Person1#: OK, please don't worry about it. You should set your mind at rest.\n",
            "#Person2#: Ha, I know you are great secretary.\n",
            "#Person1#: Thanks your flatter.<｜Assistant｜>Mr Li tells Miss Liu the result of the post\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: It's so earlier this morning, Mr. Li! You just came back yesterday, you should have rest much more.\n",
            "#Person2#: I'd like to have rest, but I have many things to do.\n",
            "#Person1#: How about this post-trip?\n",
            "#Person2#: It's very good, and the result isn't too bad.\n",
            "#Person1#: Really?\n",
            "#Person2#: Of course. I will write a report about this post-trip, and then you'd better type it at the moment, OK?\n",
            "#Person1#: OK, it's no problem.\n",
            "#Person2#: ( Afternoon ) Miss Liu. Are you typing my report?\n",
            "#Person1#: Yes, what can I do for you?\n",
            "#Person2#: Nothing. I just tell you take care of it when you type, especially the address and telephone number of those new guests. You should make sure that it's no mistaken.\n",
            "#Person1#: OK, please don't worry about it. You should set your mind at rest.\n",
            "#Person2#: Ha, I know you are great secretary.\n",
            "#Person1#: Thanks your flatter.<｜Assistant｜>Mr Li tells Miss Liu the result of the post-trip trip, and asks her to type the report.\n",
            "Miss Liu responds with information about the new guests and requests to make sure the address and telephone numbers are correct.\n",
            "Mr Li then informs her that the result is good and the report will be written.\n",
            "Miss Liu agrees to follow Mr Li's instructions and be ready.\n",
            "Mr Li thanks her for the assistance.\n",
            "</think>\n",
            "\n",
            "Mr. Li informs Miss Liu that the post-trip result is good and will write a report. She agrees to follow Mr.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 26:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Which service offered by your bank do you use most?\n",
            "#Person2#: I use several services. Of course, I deposit and withdraw money quite often. I often use my ATM card to take money out of my current account. I use my bank to exchange money from once currency to another. I often travel abroad, you see.\n",
            "#Person1#: Do you ever ask you bank for traveller's cheques? They are much safer than carrying lots of cash around.\n",
            "#Person2#: I sometimes use traveller's cheques, but sometimes I travel to countries where they are hard to exchange for cash.\n",
            "#Person1#: Do you use your bank to pay your utility bills? I use direct debit.\n",
            "#Person2#: Yes, I do. It save me a lot of time. I also have standing orders for my subscriptions to magazines.\n",
            "#Person1#: That's good idea. You don't need to worry about missing an issue of a magazine if you do that. I suppose you have a mortgage too.\n",
            "#Person2#: Yes. My bank offers very good terms and conditions on mortgage. There's a lot of competition between banks nowadays. Each one is trying to offer better conditions and services than\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Which service offered by your bank do you use most?\n",
            "#Person2#: I use several services. Of course, I deposit and withdraw money quite often. I often use my ATM card to take money out of my current account. I use my bank to exchange money from once currency to another. I often travel abroad, you see.\n",
            "#Person1#: Do you ever ask you bank for traveller's cheques? They are much safer than carrying lots of cash around.\n",
            "#Person2#: I sometimes use traveller's cheques, but sometimes I travel to countries where they are hard to exchange for cash.\n",
            "#Person1#: Do you use your bank to pay your utility bills? I use direct debit.\n",
            "#Person2#: Yes, I do. It save me a lot of time. I also have standing orders for my subscriptions to magazines.\n",
            "#Person1#: That's good idea. You don't need to worry about missing an issue of a magazine if you do that. I suppose you have a mortgage too.\n",
            "#Person2#: Yes. My bank offers very good terms and conditions on mortgage. There's a lot of competition between banks nowadays. Each one is trying to offer better conditions and services than the others.\n",
            "#Person1#: So, can you tell me which service do you have most?\n",
            "#Person2#: I use several services, mostly ATM, direct debit, and bank account.\n",
            "\n",
            "So, the conversation is about the banking services used by Person2, and it's a brief summary.\n",
            "</think>\n",
            "\n",
            "In the conversation, Person2 discusses various banking services they use, such as ATMs, direct debit, and bank accounts. They also express a preference for travel abroad and mention the safety of\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 27:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: I find it really hard to express myself in class. I have lots of ideas, but they never seem to come out of my mouth.\n",
            "#Person2#: let me give you a little advice. You know what the topics are going to be, so prepares what you would like to say in advance.\n",
            "#Person1#: I often do, but I ' m worried about the responses from other people.\n",
            "#Person2#: you have to be realistic. Not everyone is going to agree with you, but people will respect your opinion and they will respect the fact that you have contributed something to the discussion.\n",
            "#Person1#: I ' m worried that what I say will not make sense.\n",
            "#Person2#: we could chat about the topic before class. I could check that what you want to say is logical and it would also give you a chance to practice speaking.\n",
            "#Person1#: that ' s a good idea. What should I do if I am in two minds about a topic?\n",
            "#Person2#: that ' s easy. Express possibilities. People will think that you are pretty smart to be able to look at the topic from more than one angle.\n",
            "#Person1#: I didn ' t realize\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: I find it really hard to express myself in class. I have lots of ideas, but they never seem to come out of my mouth.\n",
            "#Person2#: let me give you a little advice. You know what the topics are going to be, so prepares what you would like to say in advance.\n",
            "#Person1#: I often do, but I ' m worried about the responses from other people.\n",
            "#Person2#: you have to be realistic. Not everyone is going to agree with you, but people will respect your opinion and they will respect the fact that you have contributed something to the discussion.\n",
            "#Person1#: I ' m worried that what I say will not make sense.\n",
            "#Person2#: we could chat about the topic before class. I could check that what you want to say is logical and it would also give you a chance to practice speaking.\n",
            "#Person1#: that ' s a good idea. What should I do if I am in two minds about a topic?\n",
            "#Person2#: that ' s easy. Express possibilities. People will think that you are pretty smart to be able to look at the topic from more than one angle.\n",
            "#Person1#: I didn ' t realize that. I always thought of the topic as a whole, but now I can see it from different angles.\n",
            "#Person2#: yes, you have the power to think beyond the surface. People will respect your different viewpoints.\n",
            "#Person1#: I am still worried that I won't be able to express myself clearly.\n",
            "#Person2#: as long as you take your time and practice, it will get easier.\n",
            "#Person1#: I see now. So, the next step is to practice.\n",
            "\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 28:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Hi, Amy.\n",
            "#Person2#: Hi.\n",
            "#Person1#: You look great! Have you been on vacation?\n",
            "#Person2#: Yeah, I have. I was on the West Coast.\n",
            "#Person1#: Nice! How was it?\n",
            "#Person2#: Great. I had a wonderful time. It was really nice to get away from work.\n",
            "#Person1#: Where did you go?\n",
            "#Person2#: Los Angeles and San Francisco.\n",
            "#Person1#: How did you like Los Angeles?\n",
            "#Person2#: Well, it is cleaner than I thought it would be, and there are a lot of trees, so I like it better than I thought I would. But it's huge! It's really hard to get around. The bus service is terrible, and you know, I don't like taking the underground. We had to rent a car.\n",
            "#Person1#: And was the weather good?\n",
            "#Person2#: Oh, yes, it was nice and warm in Los Angeles.\n",
            "#Person1#: What about San Francisco?\n",
            "#Person2#: The weather? It was cooler than LA, but it wasn't uncomfortable.\n",
            "#Person1#: Cool in San Francisco? That's surprising\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Hi, Amy.\n",
            "#Person2#: Hi.\n",
            "#Person1#: You look great! Have you been on vacation?\n",
            "#Person2#: Yeah, I have. I was on the West Coast.\n",
            "#Person1#: Nice! How was it?\n",
            "#Person2#: Great. I had a wonderful time. It was really nice to get away from work.\n",
            "#Person1#: Where did you go?\n",
            "#Person2#: Los Angeles and San Francisco.\n",
            "#Person1#: How did you like Los Angeles?\n",
            "#Person2#: Well, it is cleaner than I thought it would be, and there are a lot of trees, so I like it better than I thought I would. But it's huge! It's really hard to get around. The bus service is terrible, and you know, I don't like taking the underground. We had to rent a car.\n",
            "#Person1#: And was the weather good?\n",
            "#Person2#: Oh, yes, it was nice and warm in Los Angeles.\n",
            "#Person1#: What about San Francisco?\n",
            "#Person2#: The weather? It was cooler than LA, but it wasn't uncomfortable.\n",
            "#Person1#: Cool in San Francisco? That's surprising.\n",
            "#Person2#: Yeah, I didn't expect that. It was cooler, but I didn't think it was bad. The beaches are beautiful there too.\n",
            "#Person1#: How did you get around in San Francisco?\n",
            "#Person2#: We took a bus to San Francisco, but the weather was so bad there. It was hot and humid, and the buses were slow, and I had to walk a lot. So we took the subway, but that's expensive.\n",
            "#Person1\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 29:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: What a cold day! The weatherman says there will be a snowstorm today. You'd better wear warm clothes, dear.\n",
            "#Person2#: I don't believe the weatherman anymore. Last week he said there would be clear skies but it rained all week.\n",
            "#Person1#: All right. Go ahead and catch cold. In winter, that always means that it is going to snow.\n",
            "#Person2#: It doesn't make any difference. Fred is driving me to work.\n",
            "#Person1#: Just the same, you have to go out for lunch.\n",
            "#Person2#: By that time, this storm will have stopped. Look! It's partly sunny already.\n",
            "#Person1#: No, it's partly cloudy. Here are your overcoat and boots.<｜Assistant｜>#Person1# suggests #Person2# wear warm clothes as there will be a snowstorm but #Person2# refuses because #Person2# doesn't believe the weatherman.<｜Assistant｜><think>\n",
            "\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: What a cold day! The weatherman says there will be a snowstorm today. You'd better wear warm clothes, dear.\n",
            "#Person2#: I don't believe the weatherman anymore. Last week he said there would be clear skies but it rained all week.\n",
            "#Person1#: All right. Go ahead and catch cold. In winter, that always means that it is going to snow.\n",
            "#Person2#: It doesn't make any difference. Fred is driving me to work.\n",
            "#Person1#: Just the same, you have to go out for lunch.\n",
            "#Person2#: By that time, this storm will have stopped. Look! It's partly sunny already.\n",
            "#Person1#: No, it's partly cloudy. Here are your overcoat and boots.<｜Assistant｜>#Person1# suggests #Person2# wear warm clothes as there will be a snowstorm but #Person2# refuses because #Person2# doesn't believe the weatherman.<｜Assistant｜><think>\n",
            "Okay, so I'm trying to understand this conversation between Person1 and Person2. Let me break it down step by step.\n",
            "\n",
            "First, Person1 mentions it's a cold day with a snowstorm expected. He advises them to wear warm clothes. That makes sense because if it's going to snow, you need layers to stay warm.\n",
            "\n",
            "Then, Person2 says he doesn't believe the weatherman. He thinks the previous week's weatherman said there'd be clear skies but it rained all\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 30:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: How are you feeling, John?\n",
            "#Person2#: Oh, not too bad, but I've got to stay in bed.\n",
            "#Person1#: Do more exercises after you are healthy.\n",
            "#Person2#: Ok, I will. You have said that many times.\n",
            "#Person1#: Are you hungry? Do you want me to buy something for you to eat?\n",
            "#Person2#: No, there's no need. Thank you all the same.<｜Assistant｜>John has to stay in bed. #Person1# suggests exercising more, which has been said many times.<｜Assistant｜><think>\n",
            "\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: How are you feeling, John?\n",
            "#Person2#: Oh, not too bad, but I've got to stay in bed.\n",
            "#Person1#: Do more exercises after you are healthy.\n",
            "#Person2#: Ok, I will. You have said that many times.\n",
            "#Person1#: Are you hungry? Do you want me to buy something for you to eat?\n",
            "#Person2#: No, there's no need. Thank you all the same.<｜Assistant｜>John has to stay in bed. #Person1# suggests exercising more, which has been said many times.<｜Assistant｜><think>\n",
            "Okay, so I need to summarize this conversation between Person1 and Person2. Let me read through the messages again to make sure I understand what's going on.\n",
            "\n",
            "Person1 starts by asking how Person2 feels. Person2 responds by saying they don't want to stay in bed and wants to exercise more. Then Person1 asks if Person2 is hungry and suggests buying something for them to eat. Person2 says no, there's no need, and thanks them all.\n",
            "\n",
            "So, the key points\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 31:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: This is a nice set of wheels. How much did the dealer charge you for this?\n",
            "#Person2#: A lot. I probably got to moonlight for the rest of my life, but so what, it's worth it. Come on, get in. Let's take a drive.\n",
            "#Person1#: Okay then, let's go fast! Whoa! This thing can really accelerate fast.\n",
            "#Person2#: Check out what happens when I put on the brakes suddenly. It can stop quickly.\n",
            "#Person1#: Hey, just watch out for cops or you're gonna end up in the slammer. You know you tend to go fast?\n",
            "#Person2#: Yes, I know it. I went through my last set of tires fast. I even had three flat tires in two weeks.\n",
            "#Person1#: Now watch. . . some guys probably run a light and hit your car.\n",
            "#Person2#: Cut out, would you?<｜Assistant｜>#Person1# and #Person2# are trying #Person2#'s new car and #Person2# shows how well the car can run and stop. #Person1# asks #Person2# to watch out.<｜Assistant｜><think>\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: This is a nice set of wheels. How much did the dealer charge you for this?\n",
            "#Person2#: A lot. I probably got to moonlight for the rest of my life, but so what, it's worth it. Come on, get in. Let's take a drive.\n",
            "#Person1#: Okay then, let's go fast! Whoa! This thing can really accelerate fast.\n",
            "#Person2#: Check out what happens when I put on the brakes suddenly. It can stop quickly.\n",
            "#Person1#: Hey, just watch out for cops or you're gonna end up in the slammer. You know you tend to go fast?\n",
            "#Person2#: Yes, I know it. I went through my last set of tires fast. I even had three flat tires in two weeks.\n",
            "#Person1#: Now watch. . . some guys probably run a light and hit your car.\n",
            "#Person2#: Cut out, would you?<｜Assistant｜>#Person1# and #Person2# are trying #Person2#'s new car and #Person2# shows how well the car can run and stop. #Person1# asks #Person2# to watch out.<｜Assistant｜><think>\n",
            "Okay, so I'm trying to figure out how to summarize this conversation between Person1 and Person2. Let me read through it again.\n",
            "\n",
            "Person1 asks about the car's wheels, which is a bit technical. Person2 responds by saying it's expensive and mentions the car's performance. They talk about accelerating and braking, and Person1 insists on watching out for cops or getting into a slammer, implying something about the car's safety.\n",
            "\n",
            "Person2 also mentions that they had a bad experience\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 32:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: The channel boats are quite fast, aren't they?\n",
            "#Person2#: Yes, very. They make the trip in about an hour.\n",
            "#Person1#: I hope we shan't have a rough passage.\n",
            "#Person2#: No, I hope not. I want to have a deck chair and enjoy the crossing.\n",
            "#Person1#: That's a good idea.\n",
            "#Person2#: The sea seems quite calm, so there will be no fear of seasickness. Are you seasick?\n",
            "#Person1#: Yes, unfortunately. I'm very much subject to it.\n",
            "#Person2#: I'm not a very good sailor, either. My mother is a dreadfully poor sailor. In fact, she never takes a voyage unless she has to. I am better than that, and even on the Pacific. after ten days of it, I can still find my sea legs. Well, happily we shall have a smooth passage today.\n",
            "#Person1#: Yes, this little boat gets up a fine speed, and I think we'll soon get to the land.\n",
            "#Person2#: Yeah, I am of the same opinion.<｜Assistant｜>#Person1# and #Person2# are taking a trip\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: The channel boats are quite fast, aren't they?\n",
            "#Person2#: Yes, very. They make the trip in about an hour.\n",
            "#Person1#: I hope we shan't have a rough passage.\n",
            "#Person2#: No, I hope not. I want to have a deck chair and enjoy the crossing.\n",
            "#Person1#: That's a good idea.\n",
            "#Person2#: The sea seems quite calm, so there will be no fear of seasickness. Are you seasick?\n",
            "#Person1#: Yes, unfortunately. I'm very much subject to it.\n",
            "#Person2#: I'm not a very good sailor, either. My mother is a dreadfully poor sailor. In fact, she never takes a voyage unless she has to. I am better than that, and even on the Pacific. after ten days of it, I can still find my sea legs. Well, happily we shall have a smooth passage today.\n",
            "#Person1#: Yes, this little boat gets up a fine speed, and I think we'll soon get to the land.\n",
            "#Person2#: Yeah, I am of the same opinion.<｜Assistant｜>#Person1# and #Person2# are taking a trip together. They mention that the channel boats are very fast, which makes their journey quicker. #Person1# is hoping for a smooth passage, while #Person2# is also hoping for a peaceful day without seasickness. They discuss their itinerary, planning to reach their destination soon.\n",
            "</think>\n",
            "\n",
            "#Person1# and #Person2# are embarking on a journey together, discussing the smoothness of their trip. They agree that the channel boats are fast, contributing to an efficient pace. #\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 33:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: What kind of gas does this car take?\n",
            "#Person2#: I don't know. It's a rental.\n",
            "#Person1#: I will give you premium unleaded. Alright?\n",
            "#Person2#: Sure, why not?\n",
            "#Person1#: That is the pump you are in front of.\n",
            "#Person2#: Is it expensive?\n",
            "#Person1#: It is the most expensive.\n",
            "#Person2#: Well, I don't want it then. Can I move to another pump?\n",
            "#Person1#: Yes, I can give you regular unleaded. But you have to back up the car a little.\n",
            "#Person2#: Give me regular unleaded then. I will back up. Can you wash my windows for me?\n",
            "#Person1#: I'm sorry, sir. This is not a full service gas station. I will help you fill the gas, but we don't wash windows here.\n",
            "#Person2#: Really? Well, I guess I have to get used to it.<｜Assistant｜>#Person1# recommends premium unleaded gas to #Person2# but #Person2# refuses because it's expensive. So #Person1# just gives #Person2# regular unleaded.\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: What kind of gas does this car take?\n",
            "#Person2#: I don't know. It's a rental.\n",
            "#Person1#: I will give you premium unleaded. Alright?\n",
            "#Person2#: Sure, why not?\n",
            "#Person1#: That is the pump you are in front of.\n",
            "#Person2#: Is it expensive?\n",
            "#Person1#: It is the most expensive.\n",
            "#Person2#: Well, I don't want it then. Can I move to another pump?\n",
            "#Person1#: Yes, I can give you regular unleaded. But you have to back up the car a little.\n",
            "#Person2#: Give me regular unleaded then. I will back up. Can you wash my windows for me?\n",
            "#Person1#: I'm sorry, sir. This is not a full service gas station. I will help you fill the gas, but we don't wash windows here.\n",
            "#Person2#: Really? Well, I guess I have to get used to it.<｜Assistant｜>#Person1# recommends premium unleaded gas to #Person2# but #Person2# refuses because it's expensive. So #Person1# just gives #Person2# regular unleaded. However, #Person2# insists on back-packing the car and washing their windows. #Person1# explains that it's not a full service station, so they can only help with gas but not window washing.\n",
            "</think>\n",
            "\n",
            "#Person1# recommends premium unleaded gas to #Person2#. However, #Person2# declines, citing the cost. #Person1# then offers regular unleaded. Despite this, #Person2# insists on back-packing the car and washing their windows\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 34:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: May I help you?\n",
            "#Person2#: Yes, I would like to check in.\n",
            "#Person1#: Do you have a reservation?\n",
            "#Person2#: Yes, my name is Carol Tiang. That's spelled J - I - A - N - G.\n",
            "#Person1#: I'm sorry, Ma'am. I can't find anything under that name. Do you have your confirmation number?\n",
            "#Person2#: No, I don't have it with me. But I know I have a reservation. My husband made it for me last month.\n",
            "#Person1#: Well, let me look again. How do you spell your first name, Ma'am?\n",
            "#Person2#: C - A - R - O - L. Carol.\n",
            "#Person1#: I don't find a Carol in the computer for today. I have a Carol Larson here. But that reservation is for next Monday.\n",
            "#Person2#: No, that's not me. My name is Tiang, not Larson. My husband reserved for me.\n",
            "#Person1#: Are you sure he used your name, Ma'am? Maybe he reserved under his name.\n",
            "#Person2#: His name is Lin, L - I -\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: May I help you?\n",
            "#Person2#: Yes, I would like to check in.\n",
            "#Person1#: Do you have a reservation?\n",
            "#Person2#: Yes, my name is Carol Tiang. That's spelled J - I - A - N - G.\n",
            "#Person1#: I'm sorry, Ma'am. I can't find anything under that name. Do you have your confirmation number?\n",
            "#Person2#: No, I don't have it with me. But I know I have a reservation. My husband made it for me last month.\n",
            "#Person1#: Well, let me look again. How do you spell your first name, Ma'am?\n",
            "#Person2#: C - A - R - O - L. Carol.\n",
            "#Person1#: I don't find a Carol in the computer for today. I have a Carol Larson here. But that reservation is for next Monday.\n",
            "#Person2#: No, that's not me. My name is Tiang, not Larson. My husband reserved for me.\n",
            "#Person1#: Are you sure he used your name, Ma'am? Maybe he reserved under his name.\n",
            "#Person2#: His name is Lin, L - I - N. Oh, I see, he used Lin for his reservation. So his real name is Lin, but he made it for me.\n",
            "#Person1#: I need to confirm the details. How many people are there?\n",
            "#Person2#: I'm here for my own convenience. I don't have any other people. It's just me.\n",
            "#Person1#: Please let me know the arrival time.\n",
            "#Person2#: I would like to stay until 2:00 PM. That\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 35:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Well, I want to talk to you about the training schedule. At the moment you're only doing an hour of training a day, right?\n",
            "#Person2#: Yes.\n",
            "#Person1#: You need to increase that to 2 hours a day.\n",
            "#Person2#: 2 hours? What will I do during the extra hour?\n",
            "#Person1#: You will have to do some strength training exercises.\n",
            "#Person2#: It sounds tiring.\n",
            "#Person1#: Yes, but you do get 8 hours of sleep every night. Now, about your diet! First of all, lots of rice and bread, and you could only eat white meat, such as chicken and turkey. Red meat is not allowed.\n",
            "#Person2#: What? I love fast food. Does that mean I can't eat hamburgers anymore?\n",
            "#Person1#: Yes.\n",
            "#Person2#: Oh, no! And pizza?\n",
            "#Person1#: No, you shouldn't eat that either. But you must eat vegetables 3 times a day, and fruit 4 times a day.\n",
            "#Person2#: OK, I can do that.<｜Assistant｜>#Person1# talks to #Person2# about the training schedule\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Well, I want to talk to you about the training schedule. At the moment you're only doing an hour of training a day, right?\n",
            "#Person2#: Yes.\n",
            "#Person1#: You need to increase that to 2 hours a day.\n",
            "#Person2#: 2 hours? What will I do during the extra hour?\n",
            "#Person1#: You will have to do some strength training exercises.\n",
            "#Person2#: It sounds tiring.\n",
            "#Person1#: Yes, but you do get 8 hours of sleep every night. Now, about your diet! First of all, lots of rice and bread, and you could only eat white meat, such as chicken and turkey. Red meat is not allowed.\n",
            "#Person2#: What? I love fast food. Does that mean I can't eat hamburgers anymore?\n",
            "#Person1#: Yes.\n",
            "#Person2#: Oh, no! And pizza?\n",
            "#Person1#: No, you shouldn't eat that either. But you must eat vegetables 3 times a day, and fruit 4 times a day.\n",
            "#Person2#: OK, I can do that.<｜Assistant｜>#Person1# talks to #Person2# about the training schedule. They agree to increase the daily training from 1 hour to 2 hours. #Person1# asks about the extra hour, which involves strength training and ensures adequate sleep and diet. #Person2# responds by loving fast food, indicating restrictions on eating hamburgers and pizza. #Person1# agrees and suggests meal prep and reducing fast food consumption.\n",
            "</think>\n",
            "\n",
            "#Person1#: I want to discuss your training schedule. Currently, you are training for one hour daily.\n",
            "\n",
            "#Person2#:\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 36:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Is that true? \n",
            "#Person2#: I guess so. Now Anna's got four kids and another on the way. \n",
            "#Person1#: Wow. She looks good for having had that many kids. She's pregnant and wearing Prada! \n",
            "#Person2#: That's Anna for you. She keeps up the image of the stylish Italian woman. Here she comes. \n",
            "#Person1#: She's glowing. It must be the pregnancy. \n",
            "#Person2#: Yeah, or the expensive Italian facial treatments. <｜Assistant｜>#Person1# and #Person2# talk about Anna who is pregnant but still fashionable.<｜Assistant｜><think>\n",
            "\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Is that true? \n",
            "#Person2#: I guess so. Now Anna's got four kids and another on the way. \n",
            "#Person1#: Wow. She looks good for having had that many kids. She's pregnant and wearing Prada! \n",
            "#Person2#: That's Anna for you. She keeps up the image of the stylish Italian woman. Here she comes. \n",
            "#Person1#: She's glowing. It must be the pregnancy. \n",
            "#Person2#: Yeah, or the expensive Italian facial treatments. <｜Assistant｜>#Person1# and #Person2# talk about Anna who is pregnant but still fashionable.<｜Assistant｜><think>\n",
            "Okay, so I need to summarize this conversation briefly. Let me read through the text again to make sure I understand it.\n",
            "\n",
            "Person1 starts by asking if the statement is true. Then Person2 agrees and mentions that Anna has four kids and is on the way, so she's wearing Prada. Person1 responds by agreeing and then says that she looks good for having many kids and is pregnant, referencing Prada. Person2 then confirms that Anna is for him, highlighting her stylish Italian image.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 37:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: So, Monica, how do you like your new office?\n",
            "#Person2#: Well, I love it. The room is special and clean with big windows and a lot of sunshine. There are plants along the corridor. Especially, I got my own desk with computer and telephone. Such a great place!\n",
            "#Person1#: That's very good. I once turn down a job offer just because the office is awful.\n",
            "#Person2#: What was so bad about it?\n",
            "#Person1#: Small rooms, small window, a small desk shared by three people. I would have been interrupted all the time and for sure not be able to deliver good work.\n",
            "#Person2#: Sounds bad! Lucky you! You didn't take that job. I think a good office environment is also important to work efficiently.\n",
            "#Person1#: Totally agree. By the way, the number of the telephone on your desk is extension 506. You can pick up your office supplies from the supply room at the end of the corridor.<｜Assistant｜>Monica's satisfied with her new office. #Person1# tells Monica #Person1# once turned down a job offer because of the awful office. They think the office\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: So, Monica, how do you like your new office?\n",
            "#Person2#: Well, I love it. The room is special and clean with big windows and a lot of sunshine. There are plants along the corridor. Especially, I got my own desk with computer and telephone. Such a great place!\n",
            "#Person1#: That's very good. I once turn down a job offer just because the office is awful.\n",
            "#Person2#: What was so bad about it?\n",
            "#Person1#: Small rooms, small window, a small desk shared by three people. I would have been interrupted all the time and for sure not be able to deliver good work.\n",
            "#Person2#: Sounds bad! Lucky you! You didn't take that job. I think a good office environment is also important to work efficiently.\n",
            "#Person1#: Totally agree. By the way, the number of the telephone on your desk is extension 506. You can pick up your office supplies from the supply room at the end of the corridor.<｜Assistant｜>Monica's satisfied with her new office. #Person1# tells Monica #Person1# once turned down a job offer because of the awful office. They think the office is great for efficiency. #Person2# responded by sharing her office details and the telephone number. #Person1# agrees with Monica's positive feedback and gives additional advice. The overall tone is positive and encouraging.\n",
            "</think>\n",
            "\n",
            "Monica's new office is highly rated. #Person1# has successfully navigated a good office environment, having turned down a job offer due to its subpar conditions. They shared their office details, including the telephone number, and compared their office to a more efficient one\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 38:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Hi I came in on flight 513. Everyone else took their luggage off the conveyor belt, but mine didn't seem to be there.\n",
            "#Person2#: Let me check the computer. Uh-oh, Looks like your luggage is on its way to Paris.\n",
            "#Person1#: You're kidding. What am I going to do?\n",
            "#Person2#: Well, we can give you this lovely shaving kit and call you the minute your luggage arrives here. Just leave us a contact number.\n",
            "#Person1#: You can reach me at 13661306917.\n",
            "#Person2#: Thank you. I am terribly sorry about this. It very rarely happens. We do have a sophisticated tracking system, so I know we will find it soon. But we apologize for the inconvenience.\n",
            "#Person1#: That's ok. There wasn't anything important in there anyway. The shaving kit more than makes up for my lost luggage.\n",
            "#Person2#: Wow! I have been working here for 15 years and you're the first person to accept a lost luggage situation cheerfully.\n",
            "#Person1#: I'Ve had a lot worse problems than this traveling so\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Hi I came in on flight 513. Everyone else took their luggage off the conveyor belt, but mine didn't seem to be there.\n",
            "#Person2#: Let me check the computer. Uh-oh, Looks like your luggage is on its way to Paris.\n",
            "#Person1#: You're kidding. What am I going to do?\n",
            "#Person2#: Well, we can give you this lovely shaving kit and call you the minute your luggage arrives here. Just leave us a contact number.\n",
            "#Person1#: You can reach me at 13661306917.\n",
            "#Person2#: Thank you. I am terribly sorry about this. It very rarely happens. We do have a sophisticated tracking system, so I know we will find it soon. But we apologize for the inconvenience.\n",
            "#Person1#: That's ok. There wasn't anything important in there anyway. The shaving kit more than makes up for my lost luggage.\n",
            "#Person2#: Wow! I have been working here for 15 years and you're the first person to accept a lost luggage situation cheerfully.\n",
            "#Person1#: I'Ve had a lot worse problems than this traveling so I'm very sorry.\n",
            "#Person2#: Yeah, I can see. I wonder if you have any other problems?\n",
            "#Person1#: I was just trying to do my homework. I forgot everything else.\n",
            "#Person2#: Okay, that's fine. Maybe we can send you a replacement soon. Or maybe just give you a replacement kit with your current items. I don't know.\n",
            "#Person1#: Yeah, that's an option.\n",
            "#Person2#: Sounds good. I'll get\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 39:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Good morning, Oriental Electric Limited.\n",
            "#Person2#: Hello, this is Catherine. I am calling about the appointment with Mr. Liu for this afternoon. Something has just come up. I am afraid the appointment will have to be rescheduled. I am very sorry about the change.\n",
            "#Person1#: That is all right. Maybe we can make another arrangement.\n",
            "#Person2#: You are very kind to say so. Thank you. Do you think tomorrow morning would suit him?\n",
            "#Person1#: I'm afraid there is a bit of a problem. in fact, he has booked up the whole week except tomorrow afternoon.\n",
            "#Person2#: Tomorrow afternoon. Um. . . When do you think I can come and see him tomorrow afternoon?\n",
            "#Person1#: Please let me check. . . Tomorrow afternoon. . . Yes, he'll be free from 3:00 tomorrow afternoon. Do you think it's convenient for you?\n",
            "#Person2#: Yes, that's quite all right for me. Can we change it?\n",
            "#Person1#: All right. Let's change it. Tomorrow afternoon at three. I am going to schedule it for that time and inform Mr. Liu about the\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Good morning, Oriental Electric Limited.\n",
            "#Person2#: Hello, this is Catherine. I am calling about the appointment with Mr. Liu for this afternoon. Something has just come up. I am afraid the appointment will have to be rescheduled. I am very sorry about the change.\n",
            "#Person1#: That is all right. Maybe we can make another arrangement.\n",
            "#Person2#: You are very kind to say so. Thank you. Do you think tomorrow morning would suit him?\n",
            "#Person1#: I'm afraid there is a bit of a problem. in fact, he has booked up the whole week except tomorrow afternoon.\n",
            "#Person2#: Tomorrow afternoon. Um. . . When do you think I can come and see him tomorrow afternoon?\n",
            "#Person1#: Please let me check. . . Tomorrow afternoon. . . Yes, he'll be free from 3:00 tomorrow afternoon. Do you think it's convenient for you?\n",
            "#Person2#: Yes, that's quite all right for me. Can we change it?\n",
            "#Person1#: All right. Let's change it. Tomorrow afternoon at three. I am going to schedule it for that time and inform Mr. Liu about the new time.\n",
            "#Person2#: That's great. Let me do that. I'll send a message to Mr. Liu and have a representative come and see him.\n",
            "#Person1#: I have to say that I really hope it works out. He's going to be an important person here, and I want him to be happy.\n",
            "#Person2#: I agree. He is a very important person. I can't wait for the news. I will arrange for a meeting to discuss the news\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 40:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Hi, Joe! Many people in our department are leaving for Thanksgiving. What are you going to do?\n",
            "#Person2#: I think I'll just stay at home. I may see some friends and watch a few DVDs. And I probably would go to the Ottawa Car Show. What about you, Ariel? What's your plan?\n",
            "#Person1#: I am going to do some traveling with my family.\n",
            "#Person2#: Oh, where to?\n",
            "#Person1#: Toronto. And on our way, we're going to visit my aunt, Valley, in Kingston.\n",
            "#Person2#: That's exciting. How long do you plan to stay?\n",
            "#Person1#: Two days. I have a get-together party with my college friends on the tenth in Toronto. Oh, I'm afraid I've got to go now. My husband is waiting for me. And we'll have to pick up Daniel from school. Have a nice holiday, Joe!\n",
            "#Person2#: Thank you. You too!<｜Assistant｜>Ariel asks Joe about his plan for Thanksgiving. Joe says he will stay at home and may go to a car show. Ariel will travel with her family.<｜Assistant｜><think>\n",
            "\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Hi, Joe! Many people in our department are leaving for Thanksgiving. What are you going to do?\n",
            "#Person2#: I think I'll just stay at home. I may see some friends and watch a few DVDs. And I probably would go to the Ottawa Car Show. What about you, Ariel? What's your plan?\n",
            "#Person1#: I am going to do some traveling with my family.\n",
            "#Person2#: Oh, where to?\n",
            "#Person1#: Toronto. And on our way, we're going to visit my aunt, Valley, in Kingston.\n",
            "#Person2#: That's exciting. How long do you plan to stay?\n",
            "#Person1#: Two days. I have a get-together party with my college friends on the tenth in Toronto. Oh, I'm afraid I've got to go now. My husband is waiting for me. And we'll have to pick up Daniel from school. Have a nice holiday, Joe!\n",
            "#Person2#: Thank you. You too!<｜Assistant｜>Ariel asks Joe about his plan for Thanksgiving. Joe says he will stay at home and may go to a car show. Ariel will travel with her family.<｜Assistant｜><think>\n",
            "Alright, let's see. The user provided a conversation between two people, Joe and Ariel, in a workplace setting. They're asking for a summary of the conversation briefly. \n",
            "\n",
            "First, I need to understand the context. It's a workplace conversation where both Joe and Ariel are discussing their plans for the next day. Joe mentions that many department members are leaving for Thanksgiving. He's planning to stay at home, see friends, watch a few DVDs, and attend the Ottawa Car Show. He also\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 **Test 41:**\n",
            "🔹 **Input:** <｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Robbie, this new walkman is really wonderful.\n",
            "#Person2#: Richard and Marilyn bought it for me for my birthday.\n",
            "#Person1#: They are so kind. You are very lucky, Robbie, to have such a nice family.\n",
            "#Person2#: Is something wrong, Alexandra?\n",
            "#Person1#: No, nothing.\n",
            "#Person2#: Yes, there is. I can tell. What's the matter? Come on, you can tell me. What's up?\n",
            "#Person1#: I received a letter from my parents this morning. I miss them very much. The Molinas treat me so nicely and I like being with your family so much...but when I received the letter with photographs of my family, I cried.\n",
            "#Person2#: You really miss your family,don't you?\n",
            "#Person1#: Yes, I know I must seem silly. It's not like I have nobody. I like the Molinas very much, and they're so kind to me.\n",
            "#Person2#: Hey, why don't we go out for a hamburger and French fries? That'll cheer you up. And you can use my walkman.\n",
            "#Person1#: That's a good idea.\n",
            "🔹 **Model Output:** Can you summarize this conversation briefly?\n",
            "\n",
            "<｜User｜>Can you summarize this conversation briefly?\n",
            "\n",
            "#Person1#: Robbie, this new walkman is really wonderful.\n",
            "#Person2#: Richard and Marilyn bought it for me for my birthday.\n",
            "#Person1#: They are so kind. You are very lucky, Robbie, to have such a nice family.\n",
            "#Person2#: Is something wrong, Alexandra?\n",
            "#Person1#: No, nothing.\n",
            "#Person2#: Yes, there is. I can tell. What's the matter? Come on, you can tell me. What's up?\n",
            "#Person1#: I received a letter from my parents this morning. I miss them very much. The Molinas treat me so nicely and I like being with your family so much...but when I received the letter with photographs of my family, I cried.\n",
            "#Person2#: You really miss your family,don't you?\n",
            "#Person1#: Yes, I know I must seem silly. It's not like I have nobody. I like the Molinas very much, and they're so kind to me.\n",
            "#Person2#: Hey, why don't we go out for a hamburger and French fries? That'll cheer you up. And you can use my walkman.\n",
            "#Person1#: That's a good idea. I can't wait.\n",
            "#Person2#: I'm not sure, but it seems like a good idea.\n",
            "#Person1#: I'll be there in 30 minutes, let you know.\n",
            "#Person2#: Okay, I'll see you then.\n",
            "</think>\n",
            "\n",
            "The conversation starts with Robbie expressing the excitement and kindness shown by the family, particularly the Molinas. He mentions receiving a letter from his parents, expressing sadness over the loss of their family. Alexandra then suggests a meal and a walk\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-8f2dc380f4e2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moutput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2256\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3256\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3257\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3259\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m                 )\n\u001b[1;32m    576\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    578\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save LoRA  adaptors for downloading\n",
        "\n",
        "import shutil\n",
        "\n",
        "# Save the LoRA adapter\n",
        "model.save_pretrained(\"lora_adapter\")\n",
        "\n",
        "# Create a zip archive of the adapter\n",
        "shutil.make_archive(\"lora_adapter\", 'zip', \"lora_adapter\")\n",
        "\n",
        "# Download the zip file (you can replace this with your preferred download method)\n",
        "from google.colab import files\n",
        "files.download(\"lora_adapter.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "O6VMeVZn0_aN",
        "outputId": "9a17ec48-d6c5-421b-b9cb-18e414de36fa"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-7ab8ab2c91ee>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create a zip archive of the adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lora_adapter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lora_adapter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Download the zip file (you can replace this with your preferred download method)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mmake_archive\u001b[0;34m(base_name, format, root_dir, base_dir, verbose, dry_run, owner, group, logger)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_cwd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36m_make_zipfile\u001b[0;34m(base_name, base_dir, verbose, dry_run, logger, owner, group, root_dir)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                         \u001b[0marcname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marcdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                         \u001b[0mzf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marcname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"adding '%s'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1815\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m     def writestr(self, zinfo_or_arcname, data,\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfdst_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_samefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compressor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# Eğitilmiş modeli yükle\n",
        "model_path = \"trained_model\"  # Kaydettiğin modelin yolu\n",
        "\n",
        "# Eğer model gerçekten LoRA ile eğitildiyse, PeftModel olarak yüklenmeli\n",
        "model = PeftModel.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "u4KSj2jB46Ws",
        "outputId": "d2672c36-891a-4dd7-a244-a5a0e525c321"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "PeftModel.from_pretrained() missing 1 required positional argument: 'model_id'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-88ccabe4950b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Eğer model gerçekten LoRA ile eğitildiyse, PeftModel olarak yüklenmeli\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: PeftModel.from_pretrained() missing 1 required positional argument: 'model_id'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# trained_model içinden yalnızca LoRA dosyalarını al\n",
        "source_dir = \"trained_model\"\n",
        "target_dir = \"lora_adapter\"\n",
        "\n",
        "# Eğer daha önce varsa, silip temizle\n",
        "shutil.rmtree(target_dir, ignore_errors=True)\n",
        "\n",
        "# Sadece LoRA ile ilgili dosyaları yeni klasöre kopyala\n",
        "shutil.copytree(source_dir, target_dir, ignore=shutil.ignore_patterns(\"pytorch_model.bin\", \"config.json\"))\n",
        "\n",
        "print(\"✅ Sadece LoRA adaptörü başarıyla kopyalandı!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zIZmKUV5GgO",
        "outputId": "f4ea9c0e-837a-43a1-dee6-895314e5c0f8"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sadece LoRA adaptörü başarıyla kopyalandı!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls lora_adapter/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhBFpP1m6CG5",
        "outputId": "e8ba22ee-93fc-4866-b76c-a64992926fd7"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adapter_config.json        README.md                tokenizer_config.json\n",
            "adapter_model.safetensors  special_tokens_map.json  tokenizer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r lora_adapter.zip lora_adapter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEw1FLq76La1",
        "outputId": "28d36d3d-51cf-4b17-da5d-9e81f3381b18"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: lora_adapter/ (stored 0%)\n",
            "  adding: lora_adapter/adapter_config.json (deflated 53%)\n",
            "  adding: lora_adapter/tokenizer_config.json (deflated 85%)\n",
            "  adding: lora_adapter/README.md (deflated 66%)\n",
            "  adding: lora_adapter/adapter_model.safetensors (deflated 8%)\n",
            "  adding: lora_adapter/special_tokens_map.json (deflated 73%)\n",
            "  adding: lora_adapter/tokenizer.json (deflated 81%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"lora_adapter.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MVW_k2gP6yKD",
        "outputId": "80917a54-22f6-4170-9fb5-f5bb7851294e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7c5d3dc1-df28-44bc-aad2-65c4f5b6a27b\", \"lora_adapter.zip\", 6188322)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -l /content/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeAQ71vA-mRO",
        "outputId": "8924e439-2182-4ba4-dc8e-77ac5325cd90"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 6072\n",
            "drwxr-xr-x  5 root root    4096 Mar 17 09:44 \u001b[0m\u001b[01;34maya_finetuned\u001b[0m/\n",
            "drwx------  6 root root    4096 Mar 17 05:16 \u001b[01;34mdrive\u001b[0m/\n",
            "drwxr-xr-x  2 root root    4096 Mar 17 10:35 \u001b[01;34mlora_adapter\u001b[0m/\n",
            "-rw-r--r--  1 root root 6188322 Mar 17 11:42 lora_adapter.zip\n",
            "drwxr-xr-x  1 root root    4096 Mar 13 13:31 \u001b[01;34msample_data\u001b[0m/\n",
            "-rw-r--r--  1 root root    2112 Mar 17 08:14 tokenized_dataset.pkl\n",
            "drwxr-xr-x  2 root root    4096 Mar 17 10:35 \u001b[01;34mtrained_model\u001b[0m/\n",
            "drwxr-xr-x 10 root root    4096 Mar 17 08:15 \u001b[01;34mwandb\u001b[0m/\n"
          ]
        }
      ]
    }
  ]
}