# **LoRA Fine-Tuning for Conversation Summarization ðŸš€**

## **1. Project Overview**
- This project fine-tunes the **DeepSeek-R1-Distill-Qwen-1.5B** model using **LoRA (Low-Rank Adaptation)** for **conversation summarization**.
- The training process is built on **Hugging Face Transformers, PEFT, and PyTorch**.
- The dataset is available in **CSV and JSON formats**, and the **pre-trained LoRA adapter** is included for easy access.

## **2. Key Features**
- **Efficient fine-tuning** of a large language model using **LoRA-based adaptation**.
- Optimized specifically for **conversation summarization** tasks.
- **Google Colab + Google Drive** integration for easy training and deployment.
- **Modular and reusable LoRA adapters** for efficient deployment.

## **3. Technologies Used**
- **Python 3.10+**
- **Hugging Face Transformers**
- **PEFT (Parameter-Efficient Fine-Tuning)**
- **PyTorch**
- **Google Colab (For Training)**
- **DeepSeek-R1-Distill-Qwen-1.5B (Base Model)**
- **Hugging Face Datasets**

## **4. Installation**
Clone the repository and install the required dependencies:

```bash
git clone https://github.com/bilgekagansenol/deepseek-lora-finetune
cd yourproject
pip install requirements
